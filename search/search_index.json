{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build AI-powered semantic search applications txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. Traditional search systems use keywords to find data. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords. Backed by state-of-the-art machine learning models, data is transformed into vector representations for search (also known as embeddings). Innovation is happening at a rapid pace, models can understand concepts in documents, audio, images and more. Summary of txtai features: \ud83d\udd0e Large-scale similarity search with multiple index backends ( Faiss , Annoy , Hnswlib ) \ud83d\udcc4 Create embeddings for text snippets, documents, audio, images and video. Supports transformers and word vectors. \ud83d\udca1 Machine-learning pipelines to run extractive question-answering, zero-shot labeling, transcription, translation, summarization and text extraction \u21aa\ufe0f\ufe0f Workflows that join pipelines together to aggregate business logic. txtai processes can be microservices or full-fledged indexing workflows. \ud83d\udd17 API bindings for JavaScript , Java , Rust and Go \u2601\ufe0f Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes) Applications range from similarity search to complex NLP-driven data extractions to generate structured databases. The following applications are powered by txtai. Application Description paperai AI-powered literature discovery and review engine for medical/scientific papers tldrstory AI-powered understanding of headlines and story text neuspo Fact-driven, real-time sports event and news site codequestion Ask coding questions directly from the terminal txtai is built with Python 3.7+, Hugging Face Transformers , Sentence Transformers and FastAPI","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"cloud/","text":"Cloud Scalable cloud-native applications can be built with txtai. The following runtimes are supported. Container Orchestration (i.e. Kubernetes) Docker Engine Serverless Images for txtai are available on Docker Hub for CPU and GPU installs. The CPU install is recommended when GPUs aren't available given the image is half the size. The base txtai images have no models installed and models will be downloaded each time the container starts. Caching the models is recommended as that will significantly reduce container start times. This can be done a couple different ways. Create a container with the models cached Set the transformers cache environment variable and mount that volume when starting the image docker run -v < local dir>:/models -e TRANSFORMERS_CACHE = /models --rm --it <docker image> Build txtai images The txtai images found on Docker Hub are configured to support most situations. This image can be locally built with different options as desired. Examples build commands below. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/base/Dockerfile # Build Ubuntu 18.04 image running Python 3.7 docker build -t txtai --build-arg BASE_IMAGE = ubuntu:18.04 --build-arg PYTHON_VERSION = 3 .7 . # Build image with GPU support docker build -t txtai --build-arg GPU = 1 . # Build minimal image with the base txtai components docker build -t txtai --build-arg COMPONENTS = . Cache models in container images As mentioned previously, model caching is recommended to reduce container start times. The following commands demonstrate this. In all cases, it is assumed a config.yml file is present in the local directory with the desired configuration set. API This section builds a Docker image that caches models and starts an API service. The config.yml file should be configured with the desired components to expose via the API. The following is a sample config.yml file that creates an Embeddings API service # config.yml writable : true embeddings : path : sentence-transformers/nli-mpnet-base-v2 content : true The next section builds the Docker image and starts an instance. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/api/Dockerfile # CPU build docker build -t txtai-api . # GPU build docker build -t txtai-api --build-arg BASE_IMAGE = neuml/txtai-gpu . # Run docker run -p 8000 :8000 --rm -it txtai-api Service This section builds a scheduled workflow service. More on scheduled workflows can be found here. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/service/Dockerfile # CPU build docker build -t txtai-service . # GPU build docker build -t txtai-service --build-arg BASE_IMAGE = neuml/txtai-gpu . Workflow This section builds a single run workflow. Example workflows can be found here. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/workflow/Dockerfile # CPU build docker build -t txtai-workflow . # GPU build docker build -t txtai-workflow --build-arg BASE_IMAGE = neuml/txtai-gpu .","title":"Cloud"},{"location":"cloud/#cloud","text":"Scalable cloud-native applications can be built with txtai. The following runtimes are supported. Container Orchestration (i.e. Kubernetes) Docker Engine Serverless Images for txtai are available on Docker Hub for CPU and GPU installs. The CPU install is recommended when GPUs aren't available given the image is half the size. The base txtai images have no models installed and models will be downloaded each time the container starts. Caching the models is recommended as that will significantly reduce container start times. This can be done a couple different ways. Create a container with the models cached Set the transformers cache environment variable and mount that volume when starting the image docker run -v < local dir>:/models -e TRANSFORMERS_CACHE = /models --rm --it <docker image>","title":"Cloud"},{"location":"cloud/#build-txtai-images","text":"The txtai images found on Docker Hub are configured to support most situations. This image can be locally built with different options as desired. Examples build commands below. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/base/Dockerfile # Build Ubuntu 18.04 image running Python 3.7 docker build -t txtai --build-arg BASE_IMAGE = ubuntu:18.04 --build-arg PYTHON_VERSION = 3 .7 . # Build image with GPU support docker build -t txtai --build-arg GPU = 1 . # Build minimal image with the base txtai components docker build -t txtai --build-arg COMPONENTS = .","title":"Build txtai images"},{"location":"cloud/#cache-models-in-container-images","text":"As mentioned previously, model caching is recommended to reduce container start times. The following commands demonstrate this. In all cases, it is assumed a config.yml file is present in the local directory with the desired configuration set.","title":"Cache models in container images"},{"location":"cloud/#api","text":"This section builds a Docker image that caches models and starts an API service. The config.yml file should be configured with the desired components to expose via the API. The following is a sample config.yml file that creates an Embeddings API service # config.yml writable : true embeddings : path : sentence-transformers/nli-mpnet-base-v2 content : true The next section builds the Docker image and starts an instance. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/api/Dockerfile # CPU build docker build -t txtai-api . # GPU build docker build -t txtai-api --build-arg BASE_IMAGE = neuml/txtai-gpu . # Run docker run -p 8000 :8000 --rm -it txtai-api","title":"API"},{"location":"cloud/#service","text":"This section builds a scheduled workflow service. More on scheduled workflows can be found here. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/service/Dockerfile # CPU build docker build -t txtai-service . # GPU build docker build -t txtai-service --build-arg BASE_IMAGE = neuml/txtai-gpu .","title":"Service"},{"location":"cloud/#workflow","text":"This section builds a single run workflow. Example workflows can be found here. # Get Dockerfile wget https://raw.githubusercontent.com/neuml/txtai/master/docker/workflow/Dockerfile # CPU build docker build -t txtai-workflow . # GPU build docker build -t txtai-workflow --build-arg BASE_IMAGE = neuml/txtai-gpu .","title":"Workflow"},{"location":"examples/","text":"Examples The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below. Semantic Search Build semantic/similarity/vector/neural search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes What's new in txtai 4.0 Content storage, SQL, object storage, reindex and compressed indexes Pipelines Transform data with NLP-backed pipelines. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Generate image captions and detect objects Captions and object detection for images API Gallery Using txtai in JavaScript, Java, Rust and Go Workflows Efficiently process data at scale. Notebook Description Run pipeline workflows Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays Entity extraction workflows Identify entity/label combinations Workflow Scheduling Schedule workflows with cron expressions Push notifications with workflows Generate and push notifications with workflows Model Training Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more Applications Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces also provided. Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17","title":"Examples"},{"location":"examples/#examples","text":"The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.","title":"Examples"},{"location":"examples/#semantic-search","text":"Build semantic/similarity/vector/neural search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes What's new in txtai 4.0 Content storage, SQL, object storage, reindex and compressed indexes","title":"Semantic Search"},{"location":"examples/#pipelines","text":"Transform data with NLP-backed pipelines. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Generate image captions and detect objects Captions and object detection for images API Gallery Using txtai in JavaScript, Java, Rust and Go","title":"Pipelines"},{"location":"examples/#workflows","text":"Efficiently process data at scale. Notebook Description Run pipeline workflows Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays Entity extraction workflows Identify entity/label combinations Workflow Scheduling Schedule workflows with cron expressions Push notifications with workflows Generate and push notifications with workflows","title":"Workflows"},{"location":"examples/#model-training","text":"Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more","title":"Model Training"},{"location":"examples/#applications","text":"Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces also provided. Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17","title":"Applications"},{"location":"further/","text":"Further reading Introducing txtai, AI-powered semantic search built on Transformers Run machine-learning workflows to transform data and build AI-powered semantic search applications with txtai Semantic search on the cheap What's new in txtai 4.0 Tutorial series on dev.to","title":"Further Reading"},{"location":"further/#further-reading","text":"Introducing txtai, AI-powered semantic search built on Transformers Run machine-learning workflows to transform data and build AI-powered semantic search applications with txtai Semantic search on the cheap What's new in txtai 4.0 Tutorial series on dev.to","title":"Further reading"},{"location":"install/","text":"Installation The easiest way to install is via pip and PyPI pip install txtai Python 3.7+ is supported. Using a Python virtual environment is recommended. Install from source txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai Environment specific prerequisites Additional environment specific prerequisites are below. Linux Optional audio transcription requires a system library to be installed macOS Run brew install libomp see this link Windows Optional dependencies require C++ Build Tools Optional dependencies txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections. All Install all dependencies (mirrors txtai < 3.2) pip install txtai[all] API Serve txtai via a web API. pip install txtai[api] Database Additional content storage options pip install txtai[database] Model Additional non-standard models pip install txtai[model] Pipeline All pipelines - default install comes with most common pipelines. pip install txtai[pipeline] Similarity Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity] Workflow All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Installation"},{"location":"install/#installation","text":"The easiest way to install is via pip and PyPI pip install txtai Python 3.7+ is supported. Using a Python virtual environment is recommended.","title":"Installation"},{"location":"install/#install-from-source","text":"txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai","title":"Install from source"},{"location":"install/#environment-specific-prerequisites","text":"Additional environment specific prerequisites are below.","title":"Environment specific prerequisites"},{"location":"install/#linux","text":"Optional audio transcription requires a system library to be installed","title":"Linux"},{"location":"install/#macos","text":"Run brew install libomp see this link","title":"macOS"},{"location":"install/#windows","text":"Optional dependencies require C++ Build Tools","title":"Windows"},{"location":"install/#optional-dependencies","text":"txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections.","title":"Optional dependencies"},{"location":"install/#all","text":"Install all dependencies (mirrors txtai < 3.2) pip install txtai[all]","title":"All"},{"location":"install/#api","text":"Serve txtai via a web API. pip install txtai[api]","title":"API"},{"location":"install/#database","text":"Additional content storage options pip install txtai[database]","title":"Database"},{"location":"install/#model","text":"Additional non-standard models pip install txtai[model]","title":"Model"},{"location":"install/#pipeline","text":"All pipelines - default install comes with most common pipelines. pip install txtai[pipeline]","title":"Pipeline"},{"location":"install/#similarity","text":"Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity]","title":"Similarity"},{"location":"install/#workflow","text":"All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Workflow"},{"location":"why/","text":"Why txtai? In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all you need # Get started in a couple lines from txtai.embeddings import Embeddings embeddings = Embeddings ({ \"path\" : \"sentence-transformers/all-MiniLM-L6-v2\" }) embeddings . index ([( 0 , \"Correct\" , None ), ( 1 , \"Not what we hoped\" , None )]) embeddings . search ( \"positive\" , 1 ) #[(0, 0.2986203730106354)] Works well with both small and big data - scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low footprint - install additional dependencies when you need them Learn by example - notebooks cover all available functionality","title":"Why txtai?"},{"location":"why/#why-txtai","text":"In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all you need # Get started in a couple lines from txtai.embeddings import Embeddings embeddings = Embeddings ({ \"path\" : \"sentence-transformers/all-MiniLM-L6-v2\" }) embeddings . index ([( 0 , \"Correct\" , None ), ( 1 , \"Not what we hoped\" , None )]) embeddings . search ( \"positive\" , 1 ) #[(0, 0.2986203730106354)] Works well with both small and big data - scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low footprint - install additional dependencies when you need them Learn by example - notebooks cover all available functionality","title":"Why txtai?"},{"location":"api/","text":"API txtai has a full-featured API, backed by FastAPI , that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note: This configuration file enables all functionality. For memory-bound systems, splitting pipelines into multiple instances is a best practice. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named config.yml, the following command starts the API process. CONFIG = config.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details. Connect to API The default port for the API is 8000. See the uvicorn link above to change this. txtai has a number of language bindings which abstract the API (see links below). Alternatively, code can be written to connect directly to the API. Documentation for a live running instance can be found at the /docs url (i.e. http://localhost:8000/docs). The following example runs a workflow using cURL. curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"sumfrench\", \"elements\": [\"https://github.com/neuml/txtai\"]}' Local instance A local API instance can be instantiated. In this case, the API runs internally, without any network connections, providing the same consolidated functionality. This enables running txtai in Python with configuration. The configuration above can be run in Python with: from txtai.api import API app = API ( config . yml ) # Run action app . workflow ( \"sumfrench\" , [ \"https://github.com/neuml/txtai\" ]) See this link for a full list of methods . Run with containers The API can be containerized and run. This will bring up an API instance without having to install Python, txtai or any dependencies on your machine! See this section for more information . Supported language bindings The following programming languages have bindings with the txtai API: JavaScript Java Rust Go See the link below for a detailed example covering how to use the API. Notebook Description API Gallery Using txtai in JavaScript, Java, Rust and Go","title":"API"},{"location":"api/#api","text":"txtai has a full-featured API, backed by FastAPI , that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note: This configuration file enables all functionality. For memory-bound systems, splitting pipelines into multiple instances is a best practice. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named config.yml, the following command starts the API process. CONFIG = config.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details.","title":"API"},{"location":"api/#connect-to-api","text":"The default port for the API is 8000. See the uvicorn link above to change this. txtai has a number of language bindings which abstract the API (see links below). Alternatively, code can be written to connect directly to the API. Documentation for a live running instance can be found at the /docs url (i.e. http://localhost:8000/docs). The following example runs a workflow using cURL. curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"sumfrench\", \"elements\": [\"https://github.com/neuml/txtai\"]}'","title":"Connect to API"},{"location":"api/#local-instance","text":"A local API instance can be instantiated. In this case, the API runs internally, without any network connections, providing the same consolidated functionality. This enables running txtai in Python with configuration. The configuration above can be run in Python with: from txtai.api import API app = API ( config . yml ) # Run action app . workflow ( \"sumfrench\" , [ \"https://github.com/neuml/txtai\" ]) See this link for a full list of methods .","title":"Local instance"},{"location":"api/#run-with-containers","text":"The API can be containerized and run. This will bring up an API instance without having to install Python, txtai or any dependencies on your machine! See this section for more information .","title":"Run with containers"},{"location":"api/#supported-language-bindings","text":"The following programming languages have bindings with the txtai API: JavaScript Java Rust Go See the link below for a detailed example covering how to use the API. Notebook Description API Gallery Using txtai in JavaScript, Java, Rust and Go","title":"Supported language bindings"},{"location":"api/cluster/","text":"Distributed embeddings clusters The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index. See the link below for a detailed example covering distributed embeddings clusters. Notebook Description Distributed embeddings cluster Distribute an embeddings index across multiple data nodes","title":"Cluster"},{"location":"api/cluster/#distributed-embeddings-clusters","text":"The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index. See the link below for a detailed example covering distributed embeddings clusters. Notebook Description Distributed embeddings cluster Distribute an embeddings index across multiple data nodes","title":"Distributed embeddings clusters"},{"location":"api/configuration/","text":"Configuration Configuration is set through YAML. In most cases, YAML keys map to fields names in Python. The example in the previous section gave a full-featured example covering a wide array of configuration options. Each section below describes the available configuration settings. Embeddings The configuration parser expects a top level embeddings key to be present in the YAML. All embeddings configuration is supported. The following example defines an embeddings index. path : index path writable : true embeddings : path : vector model content : true Two top level settings are available to control where indexes are saved and if an index is a read-only index. path path : string Path to save and load the embeddings index. Each API instance can only access a single index at a time. writable path : boolean Determines if the input embeddings index is writable (true) or read-only (false). This allows serving a read-only index. Pipeline Pipelines are loaded as top level configuration parameters. Pipeline names are automatically detected in the YAML configuration and created upon startup. All pipelines are supported. The following example defines a series of pipelines. Note that entries below are the lower-case names of the pipeline class. caption : extractor : path : model path labels : summary : tabular : translation : Under each pipeline name, configuration settings for the pipeline can be set. Workflow Workflows are defined under a top level workflow key. Each key under the workflow key is the name of the workflow. Under that is a tasks key with each task definition. The following example defines a workflow. workflow : sumtranslate : tasks : - action : summary - action : translation schedule Schedules a workflow using a cron expression . workflow : index : schedule : cron : 0/10 * * * * * elements : [ \"api params\" ] tasks : - task : service url : api url - action : index tasks tasks : list Expects a list of workflow tasks. Each element defines a single workflow task. All task configuration is supported. A shorthand syntax for creating tasks is supported. This syntax will automatically map task strings to an action:value pair. Example below. workflow : index : tasks : - action1 - action2 Each task element supports the following additional arguments. action action : string|list Both single and multi-action tasks are supported. The action parameter works slightly different when passed via configuration. The parameter(s) needs to be converted into callable method(s). If action is a pipeline that has been defined in the current configuration, it will use that pipeline as the action. There are three special action names index , upsert and search . If index or upsert are used as the action, the task will collect workflow data elements and load them into defined the embeddings index. If search is used, the task will execute embeddings queries for each input data element. Otherwise, the action must be a path to a callable object or function. The configuration parser will resolve the function name and use that as the task action. task task : string Optionally sets the type of task to create. For example, this could be a file task or a retrieve task. If this is not specified, a generic task is created. The list of workflow tasks can be found here . args args : list Optional list of static arguments to pass to the workflow task. These are combined with workflow data to pass to each __call__ .","title":"Configuration"},{"location":"api/configuration/#configuration","text":"Configuration is set through YAML. In most cases, YAML keys map to fields names in Python. The example in the previous section gave a full-featured example covering a wide array of configuration options. Each section below describes the available configuration settings.","title":"Configuration"},{"location":"api/configuration/#embeddings","text":"The configuration parser expects a top level embeddings key to be present in the YAML. All embeddings configuration is supported. The following example defines an embeddings index. path : index path writable : true embeddings : path : vector model content : true Two top level settings are available to control where indexes are saved and if an index is a read-only index.","title":"Embeddings"},{"location":"api/configuration/#path","text":"path : string Path to save and load the embeddings index. Each API instance can only access a single index at a time.","title":"path"},{"location":"api/configuration/#writable","text":"path : boolean Determines if the input embeddings index is writable (true) or read-only (false). This allows serving a read-only index.","title":"writable"},{"location":"api/configuration/#pipeline","text":"Pipelines are loaded as top level configuration parameters. Pipeline names are automatically detected in the YAML configuration and created upon startup. All pipelines are supported. The following example defines a series of pipelines. Note that entries below are the lower-case names of the pipeline class. caption : extractor : path : model path labels : summary : tabular : translation : Under each pipeline name, configuration settings for the pipeline can be set.","title":"Pipeline"},{"location":"api/configuration/#workflow","text":"Workflows are defined under a top level workflow key. Each key under the workflow key is the name of the workflow. Under that is a tasks key with each task definition. The following example defines a workflow. workflow : sumtranslate : tasks : - action : summary - action : translation","title":"Workflow"},{"location":"api/configuration/#schedule","text":"Schedules a workflow using a cron expression . workflow : index : schedule : cron : 0/10 * * * * * elements : [ \"api params\" ] tasks : - task : service url : api url - action : index","title":"schedule"},{"location":"api/configuration/#tasks","text":"tasks : list Expects a list of workflow tasks. Each element defines a single workflow task. All task configuration is supported. A shorthand syntax for creating tasks is supported. This syntax will automatically map task strings to an action:value pair. Example below. workflow : index : tasks : - action1 - action2 Each task element supports the following additional arguments.","title":"tasks"},{"location":"api/configuration/#action","text":"action : string|list Both single and multi-action tasks are supported. The action parameter works slightly different when passed via configuration. The parameter(s) needs to be converted into callable method(s). If action is a pipeline that has been defined in the current configuration, it will use that pipeline as the action. There are three special action names index , upsert and search . If index or upsert are used as the action, the task will collect workflow data elements and load them into defined the embeddings index. If search is used, the task will execute embeddings queries for each input data element. Otherwise, the action must be a path to a callable object or function. The configuration parser will resolve the function name and use that as the task action.","title":"action"},{"location":"api/configuration/#task","text":"task : string Optionally sets the type of task to create. For example, this could be a file task or a retrieve task. If this is not specified, a generic task is created. The list of workflow tasks can be found here .","title":"task"},{"location":"api/configuration/#args","text":"args : list Optional list of static arguments to pass to the workflow task. These are combined with workflow data to pass to each __call__ .","title":"args"},{"location":"api/methods/","text":"Methods API Base API template. Downstream applications can extend this base template to add/modify functionality. Source code in txtai/api/base.py class API : \"\"\" Base API template. Downstream applications can extend this base template to add/modify functionality. \"\"\" @staticmethod def read ( data ): \"\"\" Reads a YAML configuration file. Args: data: input data Returns: yaml \"\"\" if isinstance ( data , str ): if os . path . exists ( data ): # Read yaml from file with open ( data , \"r\" , encoding = \"utf-8\" ) as f : # Read configuration return yaml . safe_load ( f ) else : # Read yaml from string return yaml . safe_load ( data ) # Return unmodified return data def __init__ ( self , config ): \"\"\" Creates an API instance, which encapsulates embeddings, pipelines and workflows. Args: config: index configuration \"\"\" # Initialize member variables self . config , self . documents , self . embeddings , self . cluster = API . read ( config ), None , None , None # Write lock - allows only a single thread to update embeddings self . lock = RLock () # ThreadPool - runs scheduled workflows self . pool = None # Local embeddings index if self . config . get ( \"path\" ) and Embeddings () . exists ( self . config [ \"path\" ]): # Load existing index if available self . embeddings = Embeddings () self . embeddings . load ( self . config [ \"path\" ]) elif self . config . get ( \"embeddings\" ): # Initialize empty embeddings self . embeddings = Embeddings ( self . config [ \"embeddings\" ]) # Embeddings cluster if self . config . get ( \"cluster\" ): self . cluster = Cluster ( self . config [ \"cluster\" ]) # Create pipelines self . pipes () # Create workflows self . flows () def __del__ ( self ): \"\"\" Close threadpool when this object is garbage collected. \"\"\" if self . pool : self . pool . close () self . pool = None def pipes ( self ): \"\"\" Initialize pipelines. \"\"\" # Pipeline definitions self . pipelines = {} # Default pipelines pipelines = list ( PipelineFactory . list () . keys ()) # Add custom pipelines for key in self . config : if \".\" in key : pipelines . append ( key ) # Create pipelines for pipeline in pipelines : if pipeline in self . config : config = self . config [ pipeline ] if self . config [ pipeline ] else {} # Custom pipeline parameters if pipeline == \"extractor\" : config [ \"similarity\" ] = self . embeddings elif pipeline == \"similarity\" and \"path\" not in config and \"labels\" in self . pipelines : config [ \"model\" ] = self . pipelines [ \"labels\" ] self . pipelines [ pipeline ] = PipelineFactory . create ( config , pipeline ) def flows ( self ): \"\"\" Initialize workflows. \"\"\" # Workflow definitions self . workflows = {} # Create workflows if \"workflow\" in self . config : for workflow , config in self . config [ \"workflow\" ] . items (): # Create copy of config config = config . copy () # Resolve callable functions config [ \"tasks\" ] = [ self . resolve ( task ) for task in config [ \"tasks\" ]] # Get scheduler config schedule = config . pop ( \"schedule\" , None ) # Create workflow self . workflows [ workflow ] = WorkflowFactory . create ( config , workflow ) # Schedule job if necessary if schedule : # Create pool if necessary if not self . pool : self . pool = ThreadPool () self . pool . apply_async ( self . workflows [ workflow ] . schedule , kwds = schedule ) def resolve ( self , task ): \"\"\" Resolves callable functions for a task. Args: task: input task config \"\"\" # Check for task shorthand syntax task = { \"action\" : task } if isinstance ( task , ( str , list )) else task if \"action\" in task : action = task [ \"action\" ] values = [ action ] if not isinstance ( action , list ) else action actions = [] for a in values : if a in [ \"index\" , \"upsert\" ]: # Add queue action to buffer documents to index actions . append ( self . add ) # Override and disable unpacking for indexing actions task [ \"unpack\" ] = False # Add finalize to trigger indexing task [ \"finalize\" ] = self . upsert if a == \"upsert\" else self . index elif a == \"search\" : actions . append ( self . batchsearch ) else : # Resolve action to callable function actions . append ( self . function ( a )) # Save resolved action(s) task [ \"action\" ] = actions [ 0 ] if not isinstance ( action , list ) else actions # Resolve initializer if \"initialize\" in task and isinstance ( task [ \"initialize\" ], str ): task [ \"initialize\" ] = self . function ( task [ \"initialize\" ]) # Resolve finalizer if \"finalize\" in task and isinstance ( task [ \"finalize\" ], str ): task [ \"finalize\" ] = self . function ( task [ \"finalize\" ]) return task def function ( self , function ): \"\"\" Get a handle to a callable function. Args: function: function name Returns: resolved function \"\"\" if function in self . pipelines : return self . pipelines [ function ] # Attempt to resolve action as a callable function return PipelineFactory . create ({}, function ) def limit ( self , limit ): \"\"\" Parses the number of results to return from the request. Allows range of 1-250, with a default of 10. Args: limit: limit parameter Returns: bounded limit \"\"\" # Return between 1 and 250 results, defaults to 10 return max ( 1 , min ( 250 , int ( limit ) if limit else 10 )) def search ( self , query , request = None , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Args: query: query text request: query request limit: maximum results, used if request is None Returns: list of {id: value, score: value} \"\"\" # When search is invoked via the API, limit is set from the request # When search is invoked directly, limit is set using the method parameter limit = self . limit ( request . query_params . get ( \"limit\" ) if request and hasattr ( request , \"query_params\" ) else limit ) if self . cluster : return self . cluster . search ( query , limit ) if self . embeddings : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. return [{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in self . embeddings . search ( query , limit )] return None def batchsearch ( self , queries , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text limit: maximum results Returns: list of {id: value, score: value} per query \"\"\" if self . cluster : return self . cluster . batchsearch ( queries , self . limit ( limit )) if self . embeddings : with self . lock : search = self . embeddings . batchsearch ( queries , self . limit ( limit )) results = [] for result in search : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. results . append ([{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in result ]) return results return None def add ( self , documents ): \"\"\" Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Args: documents: list of {id: value, text: value} Returns: unmodified input documents \"\"\" if self . cluster : self . cluster . add ( documents ) elif self . embeddings and self . config . get ( \"writable\" ): with self . lock : # Only add batch if index is marked writable # Create documents file if not already open if not self . documents : self . documents = Documents () batch = [] index = self . count () + len ( self . documents ) for document in documents : if isinstance ( document , dict ): # Pass dictionary, the embeddings instance handles parsing out the \"text\" field document = ( document [ \"id\" ], document , None ) elif isinstance ( document , str ): # Add id via autosequence document = ( index , document , None ) index += 1 elif isinstance ( document , tuple ) and len ( document ) < 3 : # Copy partial tuple document = ( document [ 0 ], document [ 1 ], None ) # Add document tuple (id, text, element) batch . append ( document ) # Add batch self . documents . add ( batch ) # Return unmodified input documents return documents def index ( self ): \"\"\" Builds an embeddings index for previously batched documents. \"\"\" if self . cluster : self . cluster . index () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Build scoring index if scoring method provided if self . embeddings . scoring : self . embeddings . score ( self . documents ) # Build embeddings index self . embeddings . index ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None def upsert ( self ): \"\"\" Runs an embeddings upsert operation for previously batched documents. \"\"\" if self . cluster : self . cluster . upsert () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Run upsert self . embeddings . upsert ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: ids deleted \"\"\" if self . cluster : return self . cluster . delete ( ids ) if self . embeddings and self . config . get ( \"writable\" ): with self . lock : return self . embeddings . delete ( ids ) return None def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in embeddings index \"\"\" if self . cluster : return self . cluster . count () if self . embeddings : return self . embeddings . count () return None def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Args: query: query text texts: list of text Returns: list of {id: value, score: value} \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"similarity\" ]( query , texts )] if self . embeddings : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . embeddings . similarity ( query , texts )] return None def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Args: queries: queries text texts: list of text Returns: list of {id: value, score: value} per query \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . pipelines [ \"similarity\" ]( queries , texts )] if self . embeddings : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . embeddings . batchsimilarity ( queries , texts )] return None def transform ( self , text ): \"\"\" Transforms text into embeddings arrays. Args: text: input text Returns: embeddings array \"\"\" if self . embeddings : return [ float ( x ) for x in self . embeddings . transform (( None , text , None ))] return None def batchtransform ( self , texts ): \"\"\" Transforms list of text into embeddings arrays. Args: texts: list of text Returns: embeddings arrays \"\"\" if self . embeddings : documents = [( None , text , None ) for text in texts ] return [[ float ( x ) for x in result ] for result in self . embeddings . batchtransform ( documents )] return None def extract ( self , queue , texts ): \"\"\" Extracts answers to input questions. Args: queue: list of {name: value, query: value, question: value, snippet: value} texts: list of text Returns: list of {name: value, answer: value} \"\"\" if self . embeddings and \"extractor\" in self . pipelines : # Convert queue to tuples queue = [( x [ \"name\" ], x [ \"query\" ], x . get ( \"question\" ), x . get ( \"snippet\" )) for x in queue ] return [{ \"name\" : name , \"answer\" : answer } for name , answer in self . pipelines [ \"extractor\" ]( queue , texts )] return None def label ( self , text , labels ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Args: text: text|list labels: list of labels Returns: list of {id: value, score: value} per text element \"\"\" if \"labels\" in self . pipelines : # Text is a string if isinstance ( text , str ): return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"labels\" ]( text , labels )] # Text is a list return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in result ] for result in self . pipelines [ \"labels\" ]( text , labels )] return None def pipeline ( self , name , args ): \"\"\" Generic pipeline execution method. Args: name: pipeline name args: pipeline arguments \"\"\" if name in self . pipelines : return self . pipelines [ name ]( * args ) return None def workflow ( self , name , elements ): \"\"\" Executes a workflow. Args: name: workflow name elements: elements to process Returns: processed elements \"\"\" # Convert lists to tuples elements = [ tuple ( element ) if isinstance ( element , list ) else element for element in elements ] # Execute workflow return self . workflows [ name ]( elements ) def wait ( self ): \"\"\" Closes threadpool and waits for completion. \"\"\" if self . pool : self . pool . close () self . pool . join () self . pool = None __init__ ( self , config ) special Creates an API instance, which encapsulates embeddings, pipelines and workflows. Parameters: Name Type Description Default config index configuration required Source code in txtai/api/base.py def __init__ ( self , config ): \"\"\" Creates an API instance, which encapsulates embeddings, pipelines and workflows. Args: config: index configuration \"\"\" # Initialize member variables self . config , self . documents , self . embeddings , self . cluster = API . read ( config ), None , None , None # Write lock - allows only a single thread to update embeddings self . lock = RLock () # ThreadPool - runs scheduled workflows self . pool = None # Local embeddings index if self . config . get ( \"path\" ) and Embeddings () . exists ( self . config [ \"path\" ]): # Load existing index if available self . embeddings = Embeddings () self . embeddings . load ( self . config [ \"path\" ]) elif self . config . get ( \"embeddings\" ): # Initialize empty embeddings self . embeddings = Embeddings ( self . config [ \"embeddings\" ]) # Embeddings cluster if self . config . get ( \"cluster\" ): self . cluster = Cluster ( self . config [ \"cluster\" ]) # Create pipelines self . pipes () # Create workflows self . flows () add ( self , documents ) Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Parameters: Name Type Description Default documents list of {id: value, text: value} required Returns: Type Description unmodified input documents Source code in txtai/api/base.py def add ( self , documents ): \"\"\" Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Args: documents: list of {id: value, text: value} Returns: unmodified input documents \"\"\" if self . cluster : self . cluster . add ( documents ) elif self . embeddings and self . config . get ( \"writable\" ): with self . lock : # Only add batch if index is marked writable # Create documents file if not already open if not self . documents : self . documents = Documents () batch = [] index = self . count () + len ( self . documents ) for document in documents : if isinstance ( document , dict ): # Pass dictionary, the embeddings instance handles parsing out the \"text\" field document = ( document [ \"id\" ], document , None ) elif isinstance ( document , str ): # Add id via autosequence document = ( index , document , None ) index += 1 elif isinstance ( document , tuple ) and len ( document ) < 3 : # Copy partial tuple document = ( document [ 0 ], document [ 1 ], None ) # Add document tuple (id, text, element) batch . append ( document ) # Add batch self . documents . add ( batch ) # Return unmodified input documents return documents batchsearch ( self , queries , limit = None ) Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text required limit maximum results None Returns: Type Description list of {id value, score: value} per query Source code in txtai/api/base.py def batchsearch ( self , queries , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text limit: maximum results Returns: list of {id: value, score: value} per query \"\"\" if self . cluster : return self . cluster . batchsearch ( queries , self . limit ( limit )) if self . embeddings : with self . lock : search = self . embeddings . batchsearch ( queries , self . limit ( limit )) results = [] for result in search : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. results . append ([{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in result ]) return results return None batchsimilarity ( self , queries , texts ) Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text required texts list of text required Returns: Type Description list of {id value, score: value} per query Source code in txtai/api/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Args: queries: queries text texts: list of text Returns: list of {id: value, score: value} per query \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . pipelines [ \"similarity\" ]( queries , texts )] if self . embeddings : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . embeddings . batchsimilarity ( queries , texts )] return None batchtransform ( self , texts ) Transforms list of text into embeddings arrays. Parameters: Name Type Description Default texts list of text required Returns: Type Description embeddings arrays Source code in txtai/api/base.py def batchtransform ( self , texts ): \"\"\" Transforms list of text into embeddings arrays. Args: texts: list of text Returns: embeddings arrays \"\"\" if self . embeddings : documents = [( None , text , None ) for text in texts ] return [[ float ( x ) for x in result ] for result in self . embeddings . batchtransform ( documents )] return None count ( self ) Total number of elements in this embeddings index. Returns: Type Description number of elements in embeddings index Source code in txtai/api/base.py def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in embeddings index \"\"\" if self . cluster : return self . cluster . count () if self . embeddings : return self . embeddings . count () return None delete ( self , ids ) Deletes from an embeddings index. Returns list of ids deleted. Parameters: Name Type Description Default ids list of ids to delete required Returns: Type Description ids deleted Source code in txtai/api/base.py def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: ids deleted \"\"\" if self . cluster : return self . cluster . delete ( ids ) if self . embeddings and self . config . get ( \"writable\" ): with self . lock : return self . embeddings . delete ( ids ) return None extract ( self , queue , texts ) Extracts answers to input questions. Parameters: Name Type Description Default queue list of {name: value, query: value, question: value, snippet: value} required texts list of text required Returns: Type Description list of {name value, answer: value} Source code in txtai/api/base.py def extract ( self , queue , texts ): \"\"\" Extracts answers to input questions. Args: queue: list of {name: value, query: value, question: value, snippet: value} texts: list of text Returns: list of {name: value, answer: value} \"\"\" if self . embeddings and \"extractor\" in self . pipelines : # Convert queue to tuples queue = [( x [ \"name\" ], x [ \"query\" ], x . get ( \"question\" ), x . get ( \"snippet\" )) for x in queue ] return [{ \"name\" : name , \"answer\" : answer } for name , answer in self . pipelines [ \"extractor\" ]( queue , texts )] return None index ( self ) Builds an embeddings index for previously batched documents. Source code in txtai/api/base.py def index ( self ): \"\"\" Builds an embeddings index for previously batched documents. \"\"\" if self . cluster : self . cluster . index () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Build scoring index if scoring method provided if self . embeddings . scoring : self . embeddings . score ( self . documents ) # Build embeddings index self . embeddings . index ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None label ( self , text , labels ) Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Parameters: Name Type Description Default text text|list required labels list of labels required Returns: Type Description list of {id value, score: value} per text element Source code in txtai/api/base.py def label ( self , text , labels ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Args: text: text|list labels: list of labels Returns: list of {id: value, score: value} per text element \"\"\" if \"labels\" in self . pipelines : # Text is a string if isinstance ( text , str ): return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"labels\" ]( text , labels )] # Text is a list return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in result ] for result in self . pipelines [ \"labels\" ]( text , labels )] return None pipeline ( self , name , args ) Generic pipeline execution method. Parameters: Name Type Description Default name pipeline name required args pipeline arguments required Source code in txtai/api/base.py def pipeline ( self , name , args ): \"\"\" Generic pipeline execution method. Args: name: pipeline name args: pipeline arguments \"\"\" if name in self . pipelines : return self . pipelines [ name ]( * args ) return None search ( self , query , request = None , limit = None ) Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Parameters: Name Type Description Default query query text required request query request None limit maximum results, used if request is None None Returns: Type Description list of {id value, score: value} Source code in txtai/api/base.py def search ( self , query , request = None , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Args: query: query text request: query request limit: maximum results, used if request is None Returns: list of {id: value, score: value} \"\"\" # When search is invoked via the API, limit is set from the request # When search is invoked directly, limit is set using the method parameter limit = self . limit ( request . query_params . get ( \"limit\" ) if request and hasattr ( request , \"query_params\" ) else limit ) if self . cluster : return self . cluster . search ( query , limit ) if self . embeddings : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. return [{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in self . embeddings . search ( query , limit )] return None similarity ( self , query , texts ) Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text required texts list of text required Returns: Type Description list of {id value, score: value} Source code in txtai/api/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Args: query: query text texts: list of text Returns: list of {id: value, score: value} \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"similarity\" ]( query , texts )] if self . embeddings : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . embeddings . similarity ( query , texts )] return None transform ( self , text ) Transforms text into embeddings arrays. Parameters: Name Type Description Default text input text required Returns: Type Description embeddings array Source code in txtai/api/base.py def transform ( self , text ): \"\"\" Transforms text into embeddings arrays. Args: text: input text Returns: embeddings array \"\"\" if self . embeddings : return [ float ( x ) for x in self . embeddings . transform (( None , text , None ))] return None upsert ( self ) Runs an embeddings upsert operation for previously batched documents. Source code in txtai/api/base.py def upsert ( self ): \"\"\" Runs an embeddings upsert operation for previously batched documents. \"\"\" if self . cluster : self . cluster . upsert () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Run upsert self . embeddings . upsert ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None wait ( self ) Closes threadpool and waits for completion. Source code in txtai/api/base.py def wait ( self ): \"\"\" Closes threadpool and waits for completion. \"\"\" if self . pool : self . pool . close () self . pool . join () self . pool = None workflow ( self , name , elements ) Executes a workflow. Parameters: Name Type Description Default name workflow name required elements elements to process required Returns: Type Description processed elements Source code in txtai/api/base.py def workflow ( self , name , elements ): \"\"\" Executes a workflow. Args: name: workflow name elements: elements to process Returns: processed elements \"\"\" # Convert lists to tuples elements = [ tuple ( element ) if isinstance ( element , list ) else element for element in elements ] # Execute workflow return self . workflows [ name ]( elements )","title":"Methods"},{"location":"api/methods/#methods","text":"","title":"Methods"},{"location":"api/methods/#txtai.api.base.API","text":"Base API template. Downstream applications can extend this base template to add/modify functionality. Source code in txtai/api/base.py class API : \"\"\" Base API template. Downstream applications can extend this base template to add/modify functionality. \"\"\" @staticmethod def read ( data ): \"\"\" Reads a YAML configuration file. Args: data: input data Returns: yaml \"\"\" if isinstance ( data , str ): if os . path . exists ( data ): # Read yaml from file with open ( data , \"r\" , encoding = \"utf-8\" ) as f : # Read configuration return yaml . safe_load ( f ) else : # Read yaml from string return yaml . safe_load ( data ) # Return unmodified return data def __init__ ( self , config ): \"\"\" Creates an API instance, which encapsulates embeddings, pipelines and workflows. Args: config: index configuration \"\"\" # Initialize member variables self . config , self . documents , self . embeddings , self . cluster = API . read ( config ), None , None , None # Write lock - allows only a single thread to update embeddings self . lock = RLock () # ThreadPool - runs scheduled workflows self . pool = None # Local embeddings index if self . config . get ( \"path\" ) and Embeddings () . exists ( self . config [ \"path\" ]): # Load existing index if available self . embeddings = Embeddings () self . embeddings . load ( self . config [ \"path\" ]) elif self . config . get ( \"embeddings\" ): # Initialize empty embeddings self . embeddings = Embeddings ( self . config [ \"embeddings\" ]) # Embeddings cluster if self . config . get ( \"cluster\" ): self . cluster = Cluster ( self . config [ \"cluster\" ]) # Create pipelines self . pipes () # Create workflows self . flows () def __del__ ( self ): \"\"\" Close threadpool when this object is garbage collected. \"\"\" if self . pool : self . pool . close () self . pool = None def pipes ( self ): \"\"\" Initialize pipelines. \"\"\" # Pipeline definitions self . pipelines = {} # Default pipelines pipelines = list ( PipelineFactory . list () . keys ()) # Add custom pipelines for key in self . config : if \".\" in key : pipelines . append ( key ) # Create pipelines for pipeline in pipelines : if pipeline in self . config : config = self . config [ pipeline ] if self . config [ pipeline ] else {} # Custom pipeline parameters if pipeline == \"extractor\" : config [ \"similarity\" ] = self . embeddings elif pipeline == \"similarity\" and \"path\" not in config and \"labels\" in self . pipelines : config [ \"model\" ] = self . pipelines [ \"labels\" ] self . pipelines [ pipeline ] = PipelineFactory . create ( config , pipeline ) def flows ( self ): \"\"\" Initialize workflows. \"\"\" # Workflow definitions self . workflows = {} # Create workflows if \"workflow\" in self . config : for workflow , config in self . config [ \"workflow\" ] . items (): # Create copy of config config = config . copy () # Resolve callable functions config [ \"tasks\" ] = [ self . resolve ( task ) for task in config [ \"tasks\" ]] # Get scheduler config schedule = config . pop ( \"schedule\" , None ) # Create workflow self . workflows [ workflow ] = WorkflowFactory . create ( config , workflow ) # Schedule job if necessary if schedule : # Create pool if necessary if not self . pool : self . pool = ThreadPool () self . pool . apply_async ( self . workflows [ workflow ] . schedule , kwds = schedule ) def resolve ( self , task ): \"\"\" Resolves callable functions for a task. Args: task: input task config \"\"\" # Check for task shorthand syntax task = { \"action\" : task } if isinstance ( task , ( str , list )) else task if \"action\" in task : action = task [ \"action\" ] values = [ action ] if not isinstance ( action , list ) else action actions = [] for a in values : if a in [ \"index\" , \"upsert\" ]: # Add queue action to buffer documents to index actions . append ( self . add ) # Override and disable unpacking for indexing actions task [ \"unpack\" ] = False # Add finalize to trigger indexing task [ \"finalize\" ] = self . upsert if a == \"upsert\" else self . index elif a == \"search\" : actions . append ( self . batchsearch ) else : # Resolve action to callable function actions . append ( self . function ( a )) # Save resolved action(s) task [ \"action\" ] = actions [ 0 ] if not isinstance ( action , list ) else actions # Resolve initializer if \"initialize\" in task and isinstance ( task [ \"initialize\" ], str ): task [ \"initialize\" ] = self . function ( task [ \"initialize\" ]) # Resolve finalizer if \"finalize\" in task and isinstance ( task [ \"finalize\" ], str ): task [ \"finalize\" ] = self . function ( task [ \"finalize\" ]) return task def function ( self , function ): \"\"\" Get a handle to a callable function. Args: function: function name Returns: resolved function \"\"\" if function in self . pipelines : return self . pipelines [ function ] # Attempt to resolve action as a callable function return PipelineFactory . create ({}, function ) def limit ( self , limit ): \"\"\" Parses the number of results to return from the request. Allows range of 1-250, with a default of 10. Args: limit: limit parameter Returns: bounded limit \"\"\" # Return between 1 and 250 results, defaults to 10 return max ( 1 , min ( 250 , int ( limit ) if limit else 10 )) def search ( self , query , request = None , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Args: query: query text request: query request limit: maximum results, used if request is None Returns: list of {id: value, score: value} \"\"\" # When search is invoked via the API, limit is set from the request # When search is invoked directly, limit is set using the method parameter limit = self . limit ( request . query_params . get ( \"limit\" ) if request and hasattr ( request , \"query_params\" ) else limit ) if self . cluster : return self . cluster . search ( query , limit ) if self . embeddings : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. return [{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in self . embeddings . search ( query , limit )] return None def batchsearch ( self , queries , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text limit: maximum results Returns: list of {id: value, score: value} per query \"\"\" if self . cluster : return self . cluster . batchsearch ( queries , self . limit ( limit )) if self . embeddings : with self . lock : search = self . embeddings . batchsearch ( queries , self . limit ( limit )) results = [] for result in search : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. results . append ([{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in result ]) return results return None def add ( self , documents ): \"\"\" Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Args: documents: list of {id: value, text: value} Returns: unmodified input documents \"\"\" if self . cluster : self . cluster . add ( documents ) elif self . embeddings and self . config . get ( \"writable\" ): with self . lock : # Only add batch if index is marked writable # Create documents file if not already open if not self . documents : self . documents = Documents () batch = [] index = self . count () + len ( self . documents ) for document in documents : if isinstance ( document , dict ): # Pass dictionary, the embeddings instance handles parsing out the \"text\" field document = ( document [ \"id\" ], document , None ) elif isinstance ( document , str ): # Add id via autosequence document = ( index , document , None ) index += 1 elif isinstance ( document , tuple ) and len ( document ) < 3 : # Copy partial tuple document = ( document [ 0 ], document [ 1 ], None ) # Add document tuple (id, text, element) batch . append ( document ) # Add batch self . documents . add ( batch ) # Return unmodified input documents return documents def index ( self ): \"\"\" Builds an embeddings index for previously batched documents. \"\"\" if self . cluster : self . cluster . index () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Build scoring index if scoring method provided if self . embeddings . scoring : self . embeddings . score ( self . documents ) # Build embeddings index self . embeddings . index ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None def upsert ( self ): \"\"\" Runs an embeddings upsert operation for previously batched documents. \"\"\" if self . cluster : self . cluster . upsert () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Run upsert self . embeddings . upsert ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: ids deleted \"\"\" if self . cluster : return self . cluster . delete ( ids ) if self . embeddings and self . config . get ( \"writable\" ): with self . lock : return self . embeddings . delete ( ids ) return None def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in embeddings index \"\"\" if self . cluster : return self . cluster . count () if self . embeddings : return self . embeddings . count () return None def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Args: query: query text texts: list of text Returns: list of {id: value, score: value} \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"similarity\" ]( query , texts )] if self . embeddings : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . embeddings . similarity ( query , texts )] return None def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Args: queries: queries text texts: list of text Returns: list of {id: value, score: value} per query \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . pipelines [ \"similarity\" ]( queries , texts )] if self . embeddings : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . embeddings . batchsimilarity ( queries , texts )] return None def transform ( self , text ): \"\"\" Transforms text into embeddings arrays. Args: text: input text Returns: embeddings array \"\"\" if self . embeddings : return [ float ( x ) for x in self . embeddings . transform (( None , text , None ))] return None def batchtransform ( self , texts ): \"\"\" Transforms list of text into embeddings arrays. Args: texts: list of text Returns: embeddings arrays \"\"\" if self . embeddings : documents = [( None , text , None ) for text in texts ] return [[ float ( x ) for x in result ] for result in self . embeddings . batchtransform ( documents )] return None def extract ( self , queue , texts ): \"\"\" Extracts answers to input questions. Args: queue: list of {name: value, query: value, question: value, snippet: value} texts: list of text Returns: list of {name: value, answer: value} \"\"\" if self . embeddings and \"extractor\" in self . pipelines : # Convert queue to tuples queue = [( x [ \"name\" ], x [ \"query\" ], x . get ( \"question\" ), x . get ( \"snippet\" )) for x in queue ] return [{ \"name\" : name , \"answer\" : answer } for name , answer in self . pipelines [ \"extractor\" ]( queue , texts )] return None def label ( self , text , labels ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Args: text: text|list labels: list of labels Returns: list of {id: value, score: value} per text element \"\"\" if \"labels\" in self . pipelines : # Text is a string if isinstance ( text , str ): return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"labels\" ]( text , labels )] # Text is a list return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in result ] for result in self . pipelines [ \"labels\" ]( text , labels )] return None def pipeline ( self , name , args ): \"\"\" Generic pipeline execution method. Args: name: pipeline name args: pipeline arguments \"\"\" if name in self . pipelines : return self . pipelines [ name ]( * args ) return None def workflow ( self , name , elements ): \"\"\" Executes a workflow. Args: name: workflow name elements: elements to process Returns: processed elements \"\"\" # Convert lists to tuples elements = [ tuple ( element ) if isinstance ( element , list ) else element for element in elements ] # Execute workflow return self . workflows [ name ]( elements ) def wait ( self ): \"\"\" Closes threadpool and waits for completion. \"\"\" if self . pool : self . pool . close () self . pool . join () self . pool = None","title":"API"},{"location":"api/methods/#txtai.api.base.API.__init__","text":"Creates an API instance, which encapsulates embeddings, pipelines and workflows. Parameters: Name Type Description Default config index configuration required Source code in txtai/api/base.py def __init__ ( self , config ): \"\"\" Creates an API instance, which encapsulates embeddings, pipelines and workflows. Args: config: index configuration \"\"\" # Initialize member variables self . config , self . documents , self . embeddings , self . cluster = API . read ( config ), None , None , None # Write lock - allows only a single thread to update embeddings self . lock = RLock () # ThreadPool - runs scheduled workflows self . pool = None # Local embeddings index if self . config . get ( \"path\" ) and Embeddings () . exists ( self . config [ \"path\" ]): # Load existing index if available self . embeddings = Embeddings () self . embeddings . load ( self . config [ \"path\" ]) elif self . config . get ( \"embeddings\" ): # Initialize empty embeddings self . embeddings = Embeddings ( self . config [ \"embeddings\" ]) # Embeddings cluster if self . config . get ( \"cluster\" ): self . cluster = Cluster ( self . config [ \"cluster\" ]) # Create pipelines self . pipes () # Create workflows self . flows ()","title":"__init__()"},{"location":"api/methods/#txtai.api.base.API.add","text":"Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Parameters: Name Type Description Default documents list of {id: value, text: value} required Returns: Type Description unmodified input documents Source code in txtai/api/base.py def add ( self , documents ): \"\"\" Adds a batch of documents for indexing. Downstream applications can override this method to also store full documents in an external system. Args: documents: list of {id: value, text: value} Returns: unmodified input documents \"\"\" if self . cluster : self . cluster . add ( documents ) elif self . embeddings and self . config . get ( \"writable\" ): with self . lock : # Only add batch if index is marked writable # Create documents file if not already open if not self . documents : self . documents = Documents () batch = [] index = self . count () + len ( self . documents ) for document in documents : if isinstance ( document , dict ): # Pass dictionary, the embeddings instance handles parsing out the \"text\" field document = ( document [ \"id\" ], document , None ) elif isinstance ( document , str ): # Add id via autosequence document = ( index , document , None ) index += 1 elif isinstance ( document , tuple ) and len ( document ) < 3 : # Copy partial tuple document = ( document [ 0 ], document [ 1 ], None ) # Add document tuple (id, text, element) batch . append ( document ) # Add batch self . documents . add ( batch ) # Return unmodified input documents return documents","title":"add()"},{"location":"api/methods/#txtai.api.base.API.batchsearch","text":"Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Parameters: Name Type Description Default queries queries text required limit maximum results None Returns: Type Description list of {id value, score: value} per query Source code in txtai/api/base.py def batchsearch ( self , queries , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model. Args: queries: queries text limit: maximum results Returns: list of {id: value, score: value} per query \"\"\" if self . cluster : return self . cluster . batchsearch ( queries , self . limit ( limit )) if self . embeddings : with self . lock : search = self . embeddings . batchsearch ( queries , self . limit ( limit )) results = [] for result in search : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. results . append ([{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in result ]) return results return None","title":"batchsearch()"},{"location":"api/methods/#txtai.api.base.API.batchsimilarity","text":"Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text required texts list of text required Returns: Type Description list of {id value, score: value} per query Source code in txtai/api/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the index in texts. Args: queries: queries text texts: list of text Returns: list of {id: value, score: value} per query \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . pipelines [ \"similarity\" ]( queries , texts )] if self . embeddings : return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in r ] for r in self . embeddings . batchsimilarity ( queries , texts )] return None","title":"batchsimilarity()"},{"location":"api/methods/#txtai.api.base.API.batchtransform","text":"Transforms list of text into embeddings arrays. Parameters: Name Type Description Default texts list of text required Returns: Type Description embeddings arrays Source code in txtai/api/base.py def batchtransform ( self , texts ): \"\"\" Transforms list of text into embeddings arrays. Args: texts: list of text Returns: embeddings arrays \"\"\" if self . embeddings : documents = [( None , text , None ) for text in texts ] return [[ float ( x ) for x in result ] for result in self . embeddings . batchtransform ( documents )] return None","title":"batchtransform()"},{"location":"api/methods/#txtai.api.base.API.count","text":"Total number of elements in this embeddings index. Returns: Type Description number of elements in embeddings index Source code in txtai/api/base.py def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in embeddings index \"\"\" if self . cluster : return self . cluster . count () if self . embeddings : return self . embeddings . count () return None","title":"count()"},{"location":"api/methods/#txtai.api.base.API.delete","text":"Deletes from an embeddings index. Returns list of ids deleted. Parameters: Name Type Description Default ids list of ids to delete required Returns: Type Description ids deleted Source code in txtai/api/base.py def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: ids deleted \"\"\" if self . cluster : return self . cluster . delete ( ids ) if self . embeddings and self . config . get ( \"writable\" ): with self . lock : return self . embeddings . delete ( ids ) return None","title":"delete()"},{"location":"api/methods/#txtai.api.base.API.extract","text":"Extracts answers to input questions. Parameters: Name Type Description Default queue list of {name: value, query: value, question: value, snippet: value} required texts list of text required Returns: Type Description list of {name value, answer: value} Source code in txtai/api/base.py def extract ( self , queue , texts ): \"\"\" Extracts answers to input questions. Args: queue: list of {name: value, query: value, question: value, snippet: value} texts: list of text Returns: list of {name: value, answer: value} \"\"\" if self . embeddings and \"extractor\" in self . pipelines : # Convert queue to tuples queue = [( x [ \"name\" ], x [ \"query\" ], x . get ( \"question\" ), x . get ( \"snippet\" )) for x in queue ] return [{ \"name\" : name , \"answer\" : answer } for name , answer in self . pipelines [ \"extractor\" ]( queue , texts )] return None","title":"extract()"},{"location":"api/methods/#txtai.api.base.API.index","text":"Builds an embeddings index for previously batched documents. Source code in txtai/api/base.py def index ( self ): \"\"\" Builds an embeddings index for previously batched documents. \"\"\" if self . cluster : self . cluster . index () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Build scoring index if scoring method provided if self . embeddings . scoring : self . embeddings . score ( self . documents ) # Build embeddings index self . embeddings . index ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None","title":"index()"},{"location":"api/methods/#txtai.api.base.API.label","text":"Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Parameters: Name Type Description Default text text|list required labels list of labels required Returns: Type Description list of {id value, score: value} per text element Source code in txtai/api/base.py def label ( self , text , labels ): \"\"\" Applies a zero shot classifier to text using a list of labels. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in labels. Args: text: text|list labels: list of labels Returns: list of {id: value, score: value} per text element \"\"\" if \"labels\" in self . pipelines : # Text is a string if isinstance ( text , str ): return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"labels\" ]( text , labels )] # Text is a list return [[{ \"id\" : uid , \"score\" : float ( score )} for uid , score in result ] for result in self . pipelines [ \"labels\" ]( text , labels )] return None","title":"label()"},{"location":"api/methods/#txtai.api.base.API.pipeline","text":"Generic pipeline execution method. Parameters: Name Type Description Default name pipeline name required args pipeline arguments required Source code in txtai/api/base.py def pipeline ( self , name , args ): \"\"\" Generic pipeline execution method. Args: name: pipeline name args: pipeline arguments \"\"\" if name in self . pipelines : return self . pipelines [ name ]( * args ) return None","title":"pipeline()"},{"location":"api/methods/#txtai.api.base.API.search","text":"Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Parameters: Name Type Description Default query query text required request query request None limit maximum results, used if request is None None Returns: Type Description list of {id value, score: value} Source code in txtai/api/base.py def search ( self , query , request = None , limit = None ): \"\"\" Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model. Downstream applications can override this method to provide enriched search results. Args: query: query text request: query request limit: maximum results, used if request is None Returns: list of {id: value, score: value} \"\"\" # When search is invoked via the API, limit is set from the request # When search is invoked directly, limit is set using the method parameter limit = self . limit ( request . query_params . get ( \"limit\" ) if request and hasattr ( request , \"query_params\" ) else limit ) if self . cluster : return self . cluster . search ( query , limit ) if self . embeddings : # Unpack (id, score) tuple, if necessary. Otherwise, results are dictionaries. return [{ \"id\" : r [ 0 ], \"score\" : float ( r [ 1 ])} if isinstance ( r , tuple ) else r for r in self . embeddings . search ( query , limit )] return None","title":"search()"},{"location":"api/methods/#txtai.api.base.API.similarity","text":"Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text required texts list of text required Returns: Type Description list of {id value, score: value} Source code in txtai/api/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of {id: value, score: value} sorted by highest score, where id is the index in texts. Args: query: query text texts: list of text Returns: list of {id: value, score: value} \"\"\" # Use similarity instance if available otherwise fall back to embeddings model if \"similarity\" in self . pipelines : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . pipelines [ \"similarity\" ]( query , texts )] if self . embeddings : return [{ \"id\" : uid , \"score\" : float ( score )} for uid , score in self . embeddings . similarity ( query , texts )] return None","title":"similarity()"},{"location":"api/methods/#txtai.api.base.API.transform","text":"Transforms text into embeddings arrays. Parameters: Name Type Description Default text input text required Returns: Type Description embeddings array Source code in txtai/api/base.py def transform ( self , text ): \"\"\" Transforms text into embeddings arrays. Args: text: input text Returns: embeddings array \"\"\" if self . embeddings : return [ float ( x ) for x in self . embeddings . transform (( None , text , None ))] return None","title":"transform()"},{"location":"api/methods/#txtai.api.base.API.upsert","text":"Runs an embeddings upsert operation for previously batched documents. Source code in txtai/api/base.py def upsert ( self ): \"\"\" Runs an embeddings upsert operation for previously batched documents. \"\"\" if self . cluster : self . cluster . upsert () elif self . embeddings and self . config . get ( \"writable\" ) and self . documents : with self . lock : # Run upsert self . embeddings . upsert ( self . documents ) # Save index if path available, otherwise this is an memory-only index if self . config . get ( \"path\" ): self . embeddings . save ( self . config [ \"path\" ]) # Reset document stream self . documents . close () self . documents = None","title":"upsert()"},{"location":"api/methods/#txtai.api.base.API.wait","text":"Closes threadpool and waits for completion. Source code in txtai/api/base.py def wait ( self ): \"\"\" Closes threadpool and waits for completion. \"\"\" if self . pool : self . pool . close () self . pool . join () self . pool = None","title":"wait()"},{"location":"api/methods/#txtai.api.base.API.workflow","text":"Executes a workflow. Parameters: Name Type Description Default name workflow name required elements elements to process required Returns: Type Description processed elements Source code in txtai/api/base.py def workflow ( self , name , elements ): \"\"\" Executes a workflow. Args: name: workflow name elements: elements to process Returns: processed elements \"\"\" # Convert lists to tuples elements = [ tuple ( element ) if isinstance ( element , list ) else element for element in elements ] # Execute workflow return self . workflows [ name ]( elements )","title":"workflow()"},{"location":"embeddings/","text":"Embeddings Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. The following code snippet shows how to build and search an embeddings index. from txtai.embeddings import Embeddings # Create embeddings model, backed by sentence-transformers & transformers embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) data = [ \"US tops 5 million confirmed virus cases\" , \"Canada's last fully intact ice shelf has suddenly collapsed, \" + \"forming a Manhattan-sized iceberg\" , \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\" , \"The National Park Service warns against sacrificing slower friends \" + \"in a bear attack\" , \"Maine man wins $1M from $25 lottery ticket\" , \"Make huge profits without work, earn up to $100,000 a day\" ] # Create an index for the list of text embeddings . index ([( uid , text , None ) for uid , text in enumerate ( data )]) print ( \" %-20s %s \" % ( \"Query\" , \"Best Match\" )) print ( \"-\" * 50 ) # Run an embeddings search for each query for query in ( \"feel good story\" , \"climate change\" , \"public health story\" , \"war\" , \"wildlife\" , \"asia\" , \"lucky\" , \"dishonest junk\" ): # Extract uid of first result # search result format: (uid, score) uid = embeddings . search ( query , 1 )[ 0 ][ 0 ] # Print text print ( \" %-20s %s \" % ( query , data [ uid ])) Build An embeddings instance can be created as follows: embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) The example above builds a transformers based embeddings instance. In this case, when loading and searching for data, a transformers model is used to vectorize data. The embeddings instance is configuration driven based on what is passed in the constructor. A number of different options are supported to cover a wide variety of use cases. Index After creating a new Embeddings instance, the next step is adding data to it. embeddings . index ([( uid , text , None ) for uid , text in enumerate ( data )]) The index method takes an iterable collection of tuples with three values. Element Description id unique record id data input data to index, can be text, a dictionary or object tags optional tags string, used to mark/label data as it's indexed The input iterable can be a list and/or a generator. Generators help with indexing very large datasets as only portions of the data is in memory at any given time. txtai buffers data to temporary storage along the way during indexing as embeddings vectors can be quite large (for example 768 dimensions of float32 is 768 * 4 = 3072 bytes per vector). Search Once data is indexed, it is ready for search. embeddings . search ( query , limit ) The search method takes two parameters, the query and query limit. The results format is different based on whether content is stored or not. List of (id, score) when content is disabled List of {**query columns} when content is enabled More examples See this link for a full list of embeddings examples.","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. The following code snippet shows how to build and search an embeddings index. from txtai.embeddings import Embeddings # Create embeddings model, backed by sentence-transformers & transformers embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) data = [ \"US tops 5 million confirmed virus cases\" , \"Canada's last fully intact ice shelf has suddenly collapsed, \" + \"forming a Manhattan-sized iceberg\" , \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\" , \"The National Park Service warns against sacrificing slower friends \" + \"in a bear attack\" , \"Maine man wins $1M from $25 lottery ticket\" , \"Make huge profits without work, earn up to $100,000 a day\" ] # Create an index for the list of text embeddings . index ([( uid , text , None ) for uid , text in enumerate ( data )]) print ( \" %-20s %s \" % ( \"Query\" , \"Best Match\" )) print ( \"-\" * 50 ) # Run an embeddings search for each query for query in ( \"feel good story\" , \"climate change\" , \"public health story\" , \"war\" , \"wildlife\" , \"asia\" , \"lucky\" , \"dishonest junk\" ): # Extract uid of first result # search result format: (uid, score) uid = embeddings . search ( query , 1 )[ 0 ][ 0 ] # Print text print ( \" %-20s %s \" % ( query , data [ uid ]))","title":"Embeddings"},{"location":"embeddings/#build","text":"An embeddings instance can be created as follows: embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) The example above builds a transformers based embeddings instance. In this case, when loading and searching for data, a transformers model is used to vectorize data. The embeddings instance is configuration driven based on what is passed in the constructor. A number of different options are supported to cover a wide variety of use cases.","title":"Build"},{"location":"embeddings/#index","text":"After creating a new Embeddings instance, the next step is adding data to it. embeddings . index ([( uid , text , None ) for uid , text in enumerate ( data )]) The index method takes an iterable collection of tuples with three values. Element Description id unique record id data input data to index, can be text, a dictionary or object tags optional tags string, used to mark/label data as it's indexed The input iterable can be a list and/or a generator. Generators help with indexing very large datasets as only portions of the data is in memory at any given time. txtai buffers data to temporary storage along the way during indexing as embeddings vectors can be quite large (for example 768 dimensions of float32 is 768 * 4 = 3072 bytes per vector).","title":"Index"},{"location":"embeddings/#search","text":"Once data is indexed, it is ready for search. embeddings . search ( query , limit ) The search method takes two parameters, the query and query limit. The results format is different based on whether content is stored or not. List of (id, score) when content is disabled List of {**query columns} when content is enabled","title":"Search"},{"location":"embeddings/#more-examples","text":"See this link for a full list of embeddings examples.","title":"More examples"},{"location":"embeddings/configuration/","text":"Configuration method method : transformers|sentence-transformers|words|external Sentence embeddings method to use. Options listed below. transformers Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings. sentence-transformers Same as transformers but loads models with the sentence-transformers library. words Builds sentence embeddings using a word embeddings model. external Sentence embeddings are loaded via an external model or API. Requires setting the transform parameter to a function that translates data into vectors. The method is inferred using the path , if not provided. sentence-transformers and words require the similarity extras package to be installed. path path : string Sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model. backend backend : faiss|hnsw|annoy Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted. faiss faiss : components : Comma separated list of components - defaults to \"Flat\" for small indices and \"IVFx,Flat\" for larger indexes where x = 4 * sqrt(embeddings count) nprobe : search probe setting (int) - defaults to x/16 (as defined above) for larger indexes See the following Faiss documentation links for more information. Guidelines for choosing an index Index configuration summary Index Factory Search Tuning hnsw hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters. annoy annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported. content content : string|boolean Enables content storage. When true, the default content storage engine will be used. Otherwise, the string must specify the supported content storage engine to use. quantize quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization. Additional configuration for Transformers models tokenize tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations. Additional configuration for Word embedding models Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed. storevectors storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies. scoring scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built. pca pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.","title":"Configuration"},{"location":"embeddings/configuration/#configuration","text":"","title":"Configuration"},{"location":"embeddings/configuration/#method","text":"method : transformers|sentence-transformers|words|external Sentence embeddings method to use. Options listed below.","title":"method"},{"location":"embeddings/configuration/#transformers","text":"Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings.","title":"transformers"},{"location":"embeddings/configuration/#sentence-transformers","text":"Same as transformers but loads models with the sentence-transformers library.","title":"sentence-transformers"},{"location":"embeddings/configuration/#words","text":"Builds sentence embeddings using a word embeddings model.","title":"words"},{"location":"embeddings/configuration/#external","text":"Sentence embeddings are loaded via an external model or API. Requires setting the transform parameter to a function that translates data into vectors. The method is inferred using the path , if not provided. sentence-transformers and words require the similarity extras package to be installed.","title":"external"},{"location":"embeddings/configuration/#path","text":"path : string Sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model.","title":"path"},{"location":"embeddings/configuration/#backend","text":"backend : faiss|hnsw|annoy Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted.","title":"backend"},{"location":"embeddings/configuration/#faiss","text":"faiss : components : Comma separated list of components - defaults to \"Flat\" for small indices and \"IVFx,Flat\" for larger indexes where x = 4 * sqrt(embeddings count) nprobe : search probe setting (int) - defaults to x/16 (as defined above) for larger indexes See the following Faiss documentation links for more information. Guidelines for choosing an index Index configuration summary Index Factory Search Tuning","title":"faiss"},{"location":"embeddings/configuration/#hnsw","text":"hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters.","title":"hnsw"},{"location":"embeddings/configuration/#annoy","text":"annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported.","title":"annoy"},{"location":"embeddings/configuration/#content","text":"content : string|boolean Enables content storage. When true, the default content storage engine will be used. Otherwise, the string must specify the supported content storage engine to use.","title":"content"},{"location":"embeddings/configuration/#quantize","text":"quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization.","title":"quantize"},{"location":"embeddings/configuration/#additional-configuration-for-transformers-models","text":"","title":"Additional configuration for Transformers models"},{"location":"embeddings/configuration/#tokenize","text":"tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations.","title":"tokenize"},{"location":"embeddings/configuration/#additional-configuration-for-word-embedding-models","text":"Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed.","title":"Additional configuration for Word embedding models"},{"location":"embeddings/configuration/#storevectors","text":"storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies.","title":"storevectors"},{"location":"embeddings/configuration/#scoring","text":"scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built.","title":"scoring"},{"location":"embeddings/configuration/#pca","text":"pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.","title":"pca"},{"location":"embeddings/methods/","text":"Methods Embeddings Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. Source code in txtai/embeddings/base.py class Embeddings : \"\"\" Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. \"\"\" # pylint: disable = W0231 def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms data into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config ) def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, data, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents ) def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, data, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Create document database, if necessary if not reindex : self . database = self . createdatabase () # Reset archive since this is a new index self . archive = None # Create transform action transform = Transform ( self , Action . REINDEX if reindex else Action . INDEX ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , dimensions , embeddings = transform ( documents , buffer ) if ids : # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Add embeddings to the index self . ann . index ( embeddings ) # Save indexids-ids mapping for indexes with no database, except when this is a reindex action if not reindex and not self . database : self . config [ \"ids\" ] = ids def upsert ( self , documents ): \"\"\" Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Args: documents: list of (id, data, tags) \"\"\" # Run standard insert if index doesn't exist if not self . ann : self . index ( documents ) return # Create transform action transform = Transform ( self , Action . UPSERT ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , _ , embeddings = transform ( documents , buffer ) if ids : # Normalize embeddings self . normalize ( embeddings ) # Append embeddings to the index self . ann . append ( embeddings ) # Save indexids-ids mapping for indexes with no database if not self . database : self . config [ \"ids\" ] = self . config [ \"ids\" ] + ids def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: list of ids deleted \"\"\" # List of internal indices for each candidate id to delete indices = [] # List of deleted ids deletes = [] if self . database : # Retrieve indexid-id mappings from database ids = self . database . ids ( ids ) # Parse out indices and ids to delete indices = [ i for i , _ in ids ] deletes = sorted ( set ( uid for _ , uid in ids )) # Delete ids from database self . database . delete ( deletes ) elif self . ann : # Lookup indexids from config for indexes with no database indexids = self . config [ \"ids\" ] # Find existing ids for uid in ids : indices . extend ([ index for index , value in enumerate ( indexids ) if uid == value ]) # Clear config ids for index in indices : deletes . append ( indexids [ index ]) indexids [ index ] = None # Delete indices from ann embeddings if indices : # Delete ids from index self . ann . delete ( indices ) return deletes def reindex ( self , config , columns = None , function = None ): \"\"\" Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Args: config: new config columns: optional list of document columns used to rebuild data function: optional function to prepare content for indexing \"\"\" if self . database : # Keep content and objects parameters to ensure database is preserved config [ \"content\" ] = self . config [ \"content\" ] if \"objects\" in self . config : config [ \"objects\" ] = self . config [ \"objects\" ] # Reset configuration self . configure ( config ) # Reindex if function : self . index ( function ( self . database . reindex ( columns )), True ) else : self . index ( self . database . reindex ( columns ), True ) def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Args: document: (id, data, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Args: documents: list of (id, data, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in this embeddings index \"\"\" return self . ann . count () if self . ann else 0 def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: input query limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" results = self . batchsearch ([ query ], limit ) return results [ 0 ] if results else results def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: input queries limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit ) def similarity ( self , query , data ): \"\"\" Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Args: query: input query data: list of data Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], data )[ 0 ] def batchsimilarity ( self , queries , data ): \"\"\" Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Args: queries: input queries data: list of data Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) data = np . array ([ self . transform (( None , row , None )) for row in data ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , data . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" ) def exists ( self , path ): \"\"\" Checks if an index exists at path. Args: path: input path Returns: True if index exists, False otherwise \"\"\" return os . path . exists ( f \" { path } /config\" ) and os . path . exists ( f \" { path } /embeddings\" ) def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = 4 ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath ) def close ( self ): \"\"\" Closes this embeddings index and frees all resources. \"\"\" self . config , self . reducer , self . scoring , self . model , self . ann , self . archive = None , None , None , None , None , None # Close database connection if open if self . database : self . database . close () self . database = None def info ( self ): \"\"\" Prints the current embeddings index configuration. \"\"\" # Copy and edit config config = self . config . copy () # Remove ids array if present config . pop ( \"ids\" , None ) # Format functions for key in config : if isinstance ( config [ key ], types . FunctionType ): config [ key ] = \"<function>\" # Print configuration print ( json . dumps ( config , sort_keys = True , indent = 2 )) def configure ( self , config ): \"\"\" Sets the configuration for this embeddings index and loads config-driven models. Args: config: embeddings configuration \"\"\" # Configuration self . config = config if self . config and self . config . get ( \"method\" ) != \"transformers\" : # Dimensionality reduction model self . reducer = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None else : self . reducer , self . scoring = None , None # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () if self . config else None def loadvectors ( self ): \"\"\" Loads a vector model set in config. Returns: vector model \"\"\" return VectorsFactory . create ( self . config , self . scoring ) def checkarchive ( self , path ): \"\"\" Checks if path is an archive file. Args: path: path to check Returns: (working directory, current path) if this is an archive, original path otherwise \"\"\" # Create archive instance, if necessary self . archive = self . archive if self . archive else Archive () # Check if path is an archive file if self . archive . isarchive ( path ): # Return temporary archive working directory and original path return self . archive . path (), path return path , None def createdatabase ( self ): \"\"\" Creates a database from config. This method will also close any existing database connection. Returns: new database, if enabled in config \"\"\" # Free existing database resources if self . database : self . database . close () # Create database from config and return return DatabaseFactory . create ( self . config ) def normalize ( self , embeddings ): \"\"\" Normalizes embeddings using L2 normalization. Operation applied directly on array. Args: embeddings: input embeddings matrix \"\"\" # Calculation is different for matrices vs vectors if len ( embeddings . shape ) > 1 : embeddings /= np . linalg . norm ( embeddings , axis = 1 )[:, np . newaxis ] else : embeddings /= np . linalg . norm ( embeddings ) __init__ ( self , config = None ) special Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms data into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config ) batchsearch ( self , queries , limit = 3 ) Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default queries input queries required limit maximum results 3 Returns: Type Description list of (id, score) per query for ann search, list of dict per query for an ann+database search Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: input queries limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit ) batchsimilarity ( self , queries , data ) Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Parameters: Name Type Description Default queries input queries required data list of data required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , data ): \"\"\" Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Args: queries: input queries data: list of data Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) data = np . array ([ self . transform (( None , row , None )) for row in data ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , data . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] batchtransform ( self , documents ) Transforms documents into embeddings vectors. Parameters: Name Type Description Default documents list of (id, data, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Args: documents: list of (id, data, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] close ( self ) Closes this embeddings index and frees all resources. Source code in txtai/embeddings/base.py def close ( self ): \"\"\" Closes this embeddings index and frees all resources. \"\"\" self . config , self . reducer , self . scoring , self . model , self . ann , self . archive = None , None , None , None , None , None # Close database connection if open if self . database : self . database . close () self . database = None count ( self ) Total number of elements in this embeddings index. Returns: Type Description number of elements in this embeddings index Source code in txtai/embeddings/base.py def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in this embeddings index \"\"\" return self . ann . count () if self . ann else 0 delete ( self , ids ) Deletes from an embeddings index. Returns list of ids deleted. Parameters: Name Type Description Default ids list of ids to delete required Returns: Type Description list of ids deleted Source code in txtai/embeddings/base.py def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: list of ids deleted \"\"\" # List of internal indices for each candidate id to delete indices = [] # List of deleted ids deletes = [] if self . database : # Retrieve indexid-id mappings from database ids = self . database . ids ( ids ) # Parse out indices and ids to delete indices = [ i for i , _ in ids ] deletes = sorted ( set ( uid for _ , uid in ids )) # Delete ids from database self . database . delete ( deletes ) elif self . ann : # Lookup indexids from config for indexes with no database indexids = self . config [ \"ids\" ] # Find existing ids for uid in ids : indices . extend ([ index for index , value in enumerate ( indexids ) if uid == value ]) # Clear config ids for index in indices : deletes . append ( indexids [ index ]) indexids [ index ] = None # Delete indices from ann embeddings if indices : # Delete ids from index self . ann . delete ( indices ) return deletes exists ( self , path ) Checks if an index exists at path. Parameters: Name Type Description Default path input path required Returns: Type Description True if index exists, False otherwise Source code in txtai/embeddings/base.py def exists ( self , path ): \"\"\" Checks if an index exists at path. Args: path: input path Returns: True if index exists, False otherwise \"\"\" return os . path . exists ( f \" { path } /config\" ) and os . path . exists ( f \" { path } /embeddings\" ) index ( self , documents , reindex = False ) Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, data, tags) required reindex if this is a reindex operation in which case database creation is skipped, defaults to False False Source code in txtai/embeddings/base.py def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, data, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Create document database, if necessary if not reindex : self . database = self . createdatabase () # Reset archive since this is a new index self . archive = None # Create transform action transform = Transform ( self , Action . REINDEX if reindex else Action . INDEX ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , dimensions , embeddings = transform ( documents , buffer ) if ids : # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Add embeddings to the index self . ann . index ( embeddings ) # Save indexids-ids mapping for indexes with no database, except when this is a reindex action if not reindex and not self . database : self . config [ \"ids\" ] = ids info ( self ) Prints the current embeddings index configuration. Source code in txtai/embeddings/base.py def info ( self ): \"\"\" Prints the current embeddings index configuration. \"\"\" # Copy and edit config config = self . config . copy () # Remove ids array if present config . pop ( \"ids\" , None ) # Format functions for key in config : if isinstance ( config [ key ], types . FunctionType ): config [ key ] = \"<function>\" # Print configuration print ( json . dumps ( config , sort_keys = True , indent = 2 )) load ( self , path ) Loads an existing index from path. Parameters: Name Type Description Default path input path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" ) reindex ( self , config , columns = None , function = None ) Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Parameters: Name Type Description Default config new config required columns optional list of document columns used to rebuild data None function optional function to prepare content for indexing None Source code in txtai/embeddings/base.py def reindex ( self , config , columns = None , function = None ): \"\"\" Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Args: config: new config columns: optional list of document columns used to rebuild data function: optional function to prepare content for indexing \"\"\" if self . database : # Keep content and objects parameters to ensure database is preserved config [ \"content\" ] = self . config [ \"content\" ] if \"objects\" in self . config : config [ \"objects\" ] = self . config [ \"objects\" ] # Reset configuration self . configure ( config ) # Reindex if function : self . index ( function ( self . database . reindex ( columns )), True ) else : self . index ( self . database . reindex ( columns ), True ) save ( self , path ) Saves an index. Parameters: Name Type Description Default path output path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = 4 ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath ) score ( self , documents ) Builds a scoring index. Only used by word vectors models. Parameters: Name Type Description Default documents list of (id, data, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, data, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents ) search ( self , query , limit = 3 ) Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default query input query required limit maximum results 3 Returns: Type Description list of (id, score) for ann search, list of dict for an ann+database search Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: input query limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" results = self . batchsearch ([ query ], limit ) return results [ 0 ] if results else results similarity ( self , query , data ) Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Parameters: Name Type Description Default query input query required data list of data required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , data ): \"\"\" Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Args: query: input query data: list of data Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], data )[ 0 ] transform ( self , document ) Transforms document into an embeddings vector. Parameters: Name Type Description Default document (id, data, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Args: document: (id, data, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding upsert ( self , documents ) Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Parameters: Name Type Description Default documents list of (id, data, tags) required Source code in txtai/embeddings/base.py def upsert ( self , documents ): \"\"\" Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Args: documents: list of (id, data, tags) \"\"\" # Run standard insert if index doesn't exist if not self . ann : self . index ( documents ) return # Create transform action transform = Transform ( self , Action . UPSERT ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , _ , embeddings = transform ( documents , buffer ) if ids : # Normalize embeddings self . normalize ( embeddings ) # Append embeddings to the index self . ann . append ( embeddings ) # Save indexids-ids mapping for indexes with no database if not self . database : self . config [ \"ids\" ] = self . config [ \"ids\" ] + ids","title":"Methods"},{"location":"embeddings/methods/#methods","text":"","title":"Methods"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings","text":"Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. Source code in txtai/embeddings/base.py class Embeddings : \"\"\" Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords. \"\"\" # pylint: disable = W0231 def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms data into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config ) def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, data, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents ) def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, data, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Create document database, if necessary if not reindex : self . database = self . createdatabase () # Reset archive since this is a new index self . archive = None # Create transform action transform = Transform ( self , Action . REINDEX if reindex else Action . INDEX ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , dimensions , embeddings = transform ( documents , buffer ) if ids : # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Add embeddings to the index self . ann . index ( embeddings ) # Save indexids-ids mapping for indexes with no database, except when this is a reindex action if not reindex and not self . database : self . config [ \"ids\" ] = ids def upsert ( self , documents ): \"\"\" Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Args: documents: list of (id, data, tags) \"\"\" # Run standard insert if index doesn't exist if not self . ann : self . index ( documents ) return # Create transform action transform = Transform ( self , Action . UPSERT ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , _ , embeddings = transform ( documents , buffer ) if ids : # Normalize embeddings self . normalize ( embeddings ) # Append embeddings to the index self . ann . append ( embeddings ) # Save indexids-ids mapping for indexes with no database if not self . database : self . config [ \"ids\" ] = self . config [ \"ids\" ] + ids def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: list of ids deleted \"\"\" # List of internal indices for each candidate id to delete indices = [] # List of deleted ids deletes = [] if self . database : # Retrieve indexid-id mappings from database ids = self . database . ids ( ids ) # Parse out indices and ids to delete indices = [ i for i , _ in ids ] deletes = sorted ( set ( uid for _ , uid in ids )) # Delete ids from database self . database . delete ( deletes ) elif self . ann : # Lookup indexids from config for indexes with no database indexids = self . config [ \"ids\" ] # Find existing ids for uid in ids : indices . extend ([ index for index , value in enumerate ( indexids ) if uid == value ]) # Clear config ids for index in indices : deletes . append ( indexids [ index ]) indexids [ index ] = None # Delete indices from ann embeddings if indices : # Delete ids from index self . ann . delete ( indices ) return deletes def reindex ( self , config , columns = None , function = None ): \"\"\" Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Args: config: new config columns: optional list of document columns used to rebuild data function: optional function to prepare content for indexing \"\"\" if self . database : # Keep content and objects parameters to ensure database is preserved config [ \"content\" ] = self . config [ \"content\" ] if \"objects\" in self . config : config [ \"objects\" ] = self . config [ \"objects\" ] # Reset configuration self . configure ( config ) # Reindex if function : self . index ( function ( self . database . reindex ( columns )), True ) else : self . index ( self . database . reindex ( columns ), True ) def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Args: document: (id, data, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Args: documents: list of (id, data, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in this embeddings index \"\"\" return self . ann . count () if self . ann else 0 def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: input query limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" results = self . batchsearch ([ query ], limit ) return results [ 0 ] if results else results def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: input queries limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit ) def similarity ( self , query , data ): \"\"\" Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Args: query: input query data: list of data Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], data )[ 0 ] def batchsimilarity ( self , queries , data ): \"\"\" Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Args: queries: input queries data: list of data Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) data = np . array ([ self . transform (( None , row , None )) for row in data ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , data . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" ) def exists ( self , path ): \"\"\" Checks if an index exists at path. Args: path: input path Returns: True if index exists, False otherwise \"\"\" return os . path . exists ( f \" { path } /config\" ) and os . path . exists ( f \" { path } /embeddings\" ) def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = 4 ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath ) def close ( self ): \"\"\" Closes this embeddings index and frees all resources. \"\"\" self . config , self . reducer , self . scoring , self . model , self . ann , self . archive = None , None , None , None , None , None # Close database connection if open if self . database : self . database . close () self . database = None def info ( self ): \"\"\" Prints the current embeddings index configuration. \"\"\" # Copy and edit config config = self . config . copy () # Remove ids array if present config . pop ( \"ids\" , None ) # Format functions for key in config : if isinstance ( config [ key ], types . FunctionType ): config [ key ] = \"<function>\" # Print configuration print ( json . dumps ( config , sort_keys = True , indent = 2 )) def configure ( self , config ): \"\"\" Sets the configuration for this embeddings index and loads config-driven models. Args: config: embeddings configuration \"\"\" # Configuration self . config = config if self . config and self . config . get ( \"method\" ) != \"transformers\" : # Dimensionality reduction model self . reducer = None # Embedding scoring method - weighs each word in a sentence self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) if self . config and self . config . get ( \"scoring\" ) else None else : self . reducer , self . scoring = None , None # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () if self . config else None def loadvectors ( self ): \"\"\" Loads a vector model set in config. Returns: vector model \"\"\" return VectorsFactory . create ( self . config , self . scoring ) def checkarchive ( self , path ): \"\"\" Checks if path is an archive file. Args: path: path to check Returns: (working directory, current path) if this is an archive, original path otherwise \"\"\" # Create archive instance, if necessary self . archive = self . archive if self . archive else Archive () # Check if path is an archive file if self . archive . isarchive ( path ): # Return temporary archive working directory and original path return self . archive . path (), path return path , None def createdatabase ( self ): \"\"\" Creates a database from config. This method will also close any existing database connection. Returns: new database, if enabled in config \"\"\" # Free existing database resources if self . database : self . database . close () # Create database from config and return return DatabaseFactory . create ( self . config ) def normalize ( self , embeddings ): \"\"\" Normalizes embeddings using L2 normalization. Operation applied directly on array. Args: embeddings: input embeddings matrix \"\"\" # Calculation is different for matrices vs vectors if len ( embeddings . shape ) > 1 : embeddings /= np . linalg . norm ( embeddings , axis = 1 )[:, np . newaxis ] else : embeddings /= np . linalg . norm ( embeddings )","title":"Embeddings"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.__init__","text":"Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms data into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config )","title":"__init__()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchsearch","text":"Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default queries input queries required limit maximum results 3 Returns: Type Description list of (id, score) per query for ann search, list of dict per query for an ann+database search Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: input queries limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit )","title":"batchsearch()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchsimilarity","text":"Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Parameters: Name Type Description Default queries input queries required data list of data required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , data ): \"\"\" Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data. Args: queries: input queries data: list of data Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) data = np . array ([ self . transform (( None , row , None )) for row in data ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , data . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ]","title":"batchsimilarity()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchtransform","text":"Transforms documents into embeddings vectors. Parameters: Name Type Description Default documents list of (id, data, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Args: documents: list of (id, data, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ]","title":"batchtransform()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.close","text":"Closes this embeddings index and frees all resources. Source code in txtai/embeddings/base.py def close ( self ): \"\"\" Closes this embeddings index and frees all resources. \"\"\" self . config , self . reducer , self . scoring , self . model , self . ann , self . archive = None , None , None , None , None , None # Close database connection if open if self . database : self . database . close () self . database = None","title":"close()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.count","text":"Total number of elements in this embeddings index. Returns: Type Description number of elements in this embeddings index Source code in txtai/embeddings/base.py def count ( self ): \"\"\" Total number of elements in this embeddings index. Returns: number of elements in this embeddings index \"\"\" return self . ann . count () if self . ann else 0","title":"count()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.delete","text":"Deletes from an embeddings index. Returns list of ids deleted. Parameters: Name Type Description Default ids list of ids to delete required Returns: Type Description list of ids deleted Source code in txtai/embeddings/base.py def delete ( self , ids ): \"\"\" Deletes from an embeddings index. Returns list of ids deleted. Args: ids: list of ids to delete Returns: list of ids deleted \"\"\" # List of internal indices for each candidate id to delete indices = [] # List of deleted ids deletes = [] if self . database : # Retrieve indexid-id mappings from database ids = self . database . ids ( ids ) # Parse out indices and ids to delete indices = [ i for i , _ in ids ] deletes = sorted ( set ( uid for _ , uid in ids )) # Delete ids from database self . database . delete ( deletes ) elif self . ann : # Lookup indexids from config for indexes with no database indexids = self . config [ \"ids\" ] # Find existing ids for uid in ids : indices . extend ([ index for index , value in enumerate ( indexids ) if uid == value ]) # Clear config ids for index in indices : deletes . append ( indexids [ index ]) indexids [ index ] = None # Delete indices from ann embeddings if indices : # Delete ids from index self . ann . delete ( indices ) return deletes","title":"delete()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.exists","text":"Checks if an index exists at path. Parameters: Name Type Description Default path input path required Returns: Type Description True if index exists, False otherwise Source code in txtai/embeddings/base.py def exists ( self , path ): \"\"\" Checks if an index exists at path. Args: path: input path Returns: True if index exists, False otherwise \"\"\" return os . path . exists ( f \" { path } /config\" ) and os . path . exists ( f \" { path } /embeddings\" )","title":"exists()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.index","text":"Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, data, tags) required reindex if this is a reindex operation in which case database creation is skipped, defaults to False False Source code in txtai/embeddings/base.py def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, data, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Create document database, if necessary if not reindex : self . database = self . createdatabase () # Reset archive since this is a new index self . archive = None # Create transform action transform = Transform ( self , Action . REINDEX if reindex else Action . INDEX ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , dimensions , embeddings = transform ( documents , buffer ) if ids : # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Add embeddings to the index self . ann . index ( embeddings ) # Save indexids-ids mapping for indexes with no database, except when this is a reindex action if not reindex and not self . database : self . config [ \"ids\" ] = ids","title":"index()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.info","text":"Prints the current embeddings index configuration. Source code in txtai/embeddings/base.py def info ( self ): \"\"\" Prints the current embeddings index configuration. \"\"\" # Copy and edit config config = self . config . copy () # Remove ids array if present config . pop ( \"ids\" , None ) # Format functions for key in config : if isinstance ( config [ key ], types . FunctionType ): config [ key ] = \"<function>\" # Print configuration print ( json . dumps ( config , sort_keys = True , indent = 2 ))","title":"info()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.load","text":"Loads an existing index from path. Parameters: Name Type Description Default path input path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms data to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" )","title":"load()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.reindex","text":"Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Parameters: Name Type Description Default config new config required columns optional list of document columns used to rebuild data None function optional function to prepare content for indexing None Source code in txtai/embeddings/base.py def reindex ( self , config , columns = None , function = None ): \"\"\" Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled. Args: config: new config columns: optional list of document columns used to rebuild data function: optional function to prepare content for indexing \"\"\" if self . database : # Keep content and objects parameters to ensure database is preserved config [ \"content\" ] = self . config [ \"content\" ] if \"objects\" in self . config : config [ \"objects\" ] = self . config [ \"objects\" ] # Reset configuration self . configure ( config ) # Reindex if function : self . index ( function ( self . database . reindex ( columns )), True ) else : self . index ( self . database . reindex ( columns ), True )","title":"reindex()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.save","text":"Saves an index. Parameters: Name Type Description Default path output path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = 4 ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath )","title":"save()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.score","text":"Builds a scoring index. Only used by word vectors models. Parameters: Name Type Description Default documents list of (id, data, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, data, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents )","title":"score()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.search","text":"Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default query input query required limit maximum results 3 Returns: Type Description list of (id, score) for ann search, list of dict for an ann+database search Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: input query limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" results = self . batchsearch ([ query ], limit ) return results [ 0 ] if results else results","title":"search()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.similarity","text":"Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Parameters: Name Type Description Default query input query required data list of data required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , data ): \"\"\" Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data. Args: query: input query data: list of data Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], data )[ 0 ]","title":"similarity()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.transform","text":"Transforms document into an embeddings vector. Parameters: Name Type Description Default document (id, data, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Args: document: (id, data, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"transform()"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.upsert","text":"Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Parameters: Name Type Description Default documents list of (id, data, tags) required Source code in txtai/embeddings/base.py def upsert ( self , documents ): \"\"\" Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation. Args: documents: list of (id, data, tags) \"\"\" # Run standard insert if index doesn't exist if not self . ann : self . index ( documents ) return # Create transform action transform = Transform ( self , Action . UPSERT ) with tempfile . NamedTemporaryFile ( mode = \"wb\" , suffix = \".npy\" ) as buffer : # Load documents into database and transform to vectors ids , _ , embeddings = transform ( documents , buffer ) if ids : # Normalize embeddings self . normalize ( embeddings ) # Append embeddings to the index self . ann . append ( embeddings ) # Save indexids-ids mapping for indexes with no database if not self . database : self . config [ \"ids\" ] = self . config [ \"ids\" ] + ids","title":"upsert()"},{"location":"embeddings/query/","text":"Query guide This section covers how to query data with txtai. The simplest way to search for data is building a natural language string with the desired content to find. txtai also supports querying with SQL. We'll cover both methods here. Natural language queries In the simplest case, the query is text and the results are index text that is most similar to the query text. embeddings . search ( \"feel good story\" ) embeddings . search ( \"wildlife\" ) The queries above search the index for similarity matches on feel good story and wildlife . If content storage is enabled, a list of {**query columns} is returned. Otherwise, a list of (id, score) tuples are returned. SQL txtai supports more complex queries with SQL. This is only supported if content storage is enabled. txtai has a translation layer that analyzes input SQL statements and combines similarity results with content stored in a relational database. SQL queries are run through embeddings.search like natural language queries but the examples below only show the SQL query for conciseness. embeddings . search ( \"SQL query\" ) Similar clause The similar clause is a txtai function that enables similarity searches with SQL. SELECT id , text , score FROM txtai WHERE similar ( 'feel good story' ) SELECT id , text , score FROM txtai WHERE similar ( 'feel good story' ) The similar clause takes two arguments: similar ( \"query\" , \"number of candidates\" ) Argument Description query natural language query to run number of candidates number of candidate results to return The txtai query layer has to join results from two separate components, a relational store and a similarity index. With a similar clause, a similarity search is run and those ids are fed to the underlying database query. The number of candidates should be larger than the desired number of results as additional filtering could occur. If this is not specified, it defaults to 10 times the number of desired results. Dynamic columns Content can be indexed in multiple ways when content storage is enabled. Remember that input documents take the form of (id, data, tags) tuples. If data is a string, then content is primarily filtered with similar clauses. If data is a dictionary, then all fields in the dictionary are indexed and searchable. For example: embeddings . index ([( 0 , { \"text\" : \"text to index\" , \"flag\" : True , \"entry\" : \"2022-01-01\" }, None )]) With the above input data, queries can now have more complex filters. SELECT text , flag , entry FROM txtai WHERE similar ( 'query' ) AND flag = 1 AND entry >= '2022-01-01' txtai's query layer automatically detects columns and translates queries into a format that can be understood by the underlying database. Nested dictionaries/JSON is supported and can be escaped with bracket statements. embeddings . index ([( 0 , { \"text\" : \"text to index\" , \"parent\" : { \"child element\" : \"abc\" }}, None )]) SELECT text FROM txtai WHERE [ parent . child element ] = 'abc' Note the bracket statement escaping the nested column with spaces in the name. Aggregation queries The goal of txtai's query language is to closely support all functions in the underlying database engine. The main challenge is ensuring dynamic columns are properly escaped into the engines native query function. Aggregation query examples. SELECT count ( * ) FROM txtai WHERE similar ( 'feel good story' ) AND score >= 0 . 15 SELECT max ( length ( text )) FROM txtai WHERE similar ( 'feel good story' ) AND score >= 0 . 15 SELECT count ( * ), flag FROM txtai GROUP BY flag ORDER BY count ( * ) DESC Binary objects txtai has support for storing and retrieving binary objects. Binary objects can be retrieved as shown in the example below. # Get an image request = open ( \"demo.gif\" ) # Insert record embeddings . index ([( \"txtai\" , { \"text\" : \"txtai executes machine-learning workflows.\" , \"object\" : request . read ()}, None )]) # Query txtai and get associated object query = \"select object from txtai where similar('machine learning') limit 1\" result = embeddings . search ( query )[ 0 ][ \"object\" ] Combined index architecture When content storage is enabled, txtai becomes a dual storage engine. Content is stored in an underlying database (currently supports SQLite) along with an Approximate Nearest Neighbor (ANN) index. These components combine to deliver similarity search alongside traditional structured search. The ANN index stores ids and vectors for each input element. When a natural language query is run, the query is translated into a vector and a similarity query finds the best matching ids. When a database is added into the mix, an additional step is applied. This step takes those ids and effectively inserts them as part of the underlying database query. Dynamic columns are supported via the underlying engine. For SQLite, data is stored as JSON and dynamic columns are converted into json_extract clauses. This same concept can be expanded to other storage engines like PostgreSQL and could even work with NoSQL stores.","title":"Query Guide"},{"location":"embeddings/query/#query-guide","text":"This section covers how to query data with txtai. The simplest way to search for data is building a natural language string with the desired content to find. txtai also supports querying with SQL. We'll cover both methods here.","title":"Query guide"},{"location":"embeddings/query/#natural-language-queries","text":"In the simplest case, the query is text and the results are index text that is most similar to the query text. embeddings . search ( \"feel good story\" ) embeddings . search ( \"wildlife\" ) The queries above search the index for similarity matches on feel good story and wildlife . If content storage is enabled, a list of {**query columns} is returned. Otherwise, a list of (id, score) tuples are returned.","title":"Natural language queries"},{"location":"embeddings/query/#sql","text":"txtai supports more complex queries with SQL. This is only supported if content storage is enabled. txtai has a translation layer that analyzes input SQL statements and combines similarity results with content stored in a relational database. SQL queries are run through embeddings.search like natural language queries but the examples below only show the SQL query for conciseness. embeddings . search ( \"SQL query\" )","title":"SQL"},{"location":"embeddings/query/#similar-clause","text":"The similar clause is a txtai function that enables similarity searches with SQL. SELECT id , text , score FROM txtai WHERE similar ( 'feel good story' ) SELECT id , text , score FROM txtai WHERE similar ( 'feel good story' ) The similar clause takes two arguments: similar ( \"query\" , \"number of candidates\" ) Argument Description query natural language query to run number of candidates number of candidate results to return The txtai query layer has to join results from two separate components, a relational store and a similarity index. With a similar clause, a similarity search is run and those ids are fed to the underlying database query. The number of candidates should be larger than the desired number of results as additional filtering could occur. If this is not specified, it defaults to 10 times the number of desired results.","title":"Similar clause"},{"location":"embeddings/query/#dynamic-columns","text":"Content can be indexed in multiple ways when content storage is enabled. Remember that input documents take the form of (id, data, tags) tuples. If data is a string, then content is primarily filtered with similar clauses. If data is a dictionary, then all fields in the dictionary are indexed and searchable. For example: embeddings . index ([( 0 , { \"text\" : \"text to index\" , \"flag\" : True , \"entry\" : \"2022-01-01\" }, None )]) With the above input data, queries can now have more complex filters. SELECT text , flag , entry FROM txtai WHERE similar ( 'query' ) AND flag = 1 AND entry >= '2022-01-01' txtai's query layer automatically detects columns and translates queries into a format that can be understood by the underlying database. Nested dictionaries/JSON is supported and can be escaped with bracket statements. embeddings . index ([( 0 , { \"text\" : \"text to index\" , \"parent\" : { \"child element\" : \"abc\" }}, None )]) SELECT text FROM txtai WHERE [ parent . child element ] = 'abc' Note the bracket statement escaping the nested column with spaces in the name.","title":"Dynamic columns"},{"location":"embeddings/query/#aggregation-queries","text":"The goal of txtai's query language is to closely support all functions in the underlying database engine. The main challenge is ensuring dynamic columns are properly escaped into the engines native query function. Aggregation query examples. SELECT count ( * ) FROM txtai WHERE similar ( 'feel good story' ) AND score >= 0 . 15 SELECT max ( length ( text )) FROM txtai WHERE similar ( 'feel good story' ) AND score >= 0 . 15 SELECT count ( * ), flag FROM txtai GROUP BY flag ORDER BY count ( * ) DESC","title":"Aggregation queries"},{"location":"embeddings/query/#binary-objects","text":"txtai has support for storing and retrieving binary objects. Binary objects can be retrieved as shown in the example below. # Get an image request = open ( \"demo.gif\" ) # Insert record embeddings . index ([( \"txtai\" , { \"text\" : \"txtai executes machine-learning workflows.\" , \"object\" : request . read ()}, None )]) # Query txtai and get associated object query = \"select object from txtai where similar('machine learning') limit 1\" result = embeddings . search ( query )[ 0 ][ \"object\" ]","title":"Binary objects"},{"location":"embeddings/query/#combined-index-architecture","text":"When content storage is enabled, txtai becomes a dual storage engine. Content is stored in an underlying database (currently supports SQLite) along with an Approximate Nearest Neighbor (ANN) index. These components combine to deliver similarity search alongside traditional structured search. The ANN index stores ids and vectors for each input element. When a natural language query is run, the query is translated into a vector and a similarity query finds the best matching ids. When a database is added into the mix, an additional step is applied. This step takes those ids and effectively inserts them as part of the underlying database query. Dynamic columns are supported via the underlying engine. For SQLite, data is stored as JSON and dynamic columns are converted into json_extract clauses. This same concept can be expanded to other storage engines like PostgreSQL and could even work with NoSQL stores.","title":"Combined index architecture"},{"location":"pipeline/","text":"Pipeline txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and process various types of data. Pipelines can wrap machine learning models as well as other processes. Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . List of pipelines The following is a list of the default pipelines available in txtai. Audio Transcription Data Processing Segmentation Tabular Text extraction Image Caption Objects Text Entity Extractive QA Labeling Similarity Summary Translation Training HF ONNX ML ONNX Trainer","title":"Pipeline"},{"location":"pipeline/#pipeline","text":"txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and process various types of data. Pipelines can wrap machine learning models as well as other processes. Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Pipeline"},{"location":"pipeline/#list-of-pipelines","text":"The following is a list of the default pipelines available in txtai. Audio Transcription Data Processing Segmentation Tabular Text extraction Image Caption Objects Text Entity Extractive QA Labeling Similarity Summary Translation Training HF ONNX ML ONNX Trainer","title":"List of pipelines"},{"location":"pipeline/audio/transcription/","text":"Transcription The Transcription pipeline converts speech in audio files to text. Example The following shows a simple example using this pipeline. from txtai.pipeline import Transcription # Create and run pipeline transcribe = Transcription () transcribe ( \"path to wav file\" ) See the link below for a more detailed example. Notebook Description Transcribe audio to text Convert audio files to text Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name transcription : # Run pipeline with workflow workflow : transcribe : tasks : - action : transcription Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"transcribe\" , [ \"path to wav file\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"transcribe\", \"elements\":[\"path to wav file\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = 'facebook/wav2vec2-base-960h' , quantize = False , gpu = True , batch = 64 ) special Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/audio/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , files ) special Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/audio/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"Transcription"},{"location":"pipeline/audio/transcription/#transcription","text":"The Transcription pipeline converts speech in audio files to text.","title":"Transcription"},{"location":"pipeline/audio/transcription/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Transcription # Create and run pipeline transcribe = Transcription () transcribe ( \"path to wav file\" ) See the link below for a more detailed example. Notebook Description Transcribe audio to text Convert audio files to text","title":"Example"},{"location":"pipeline/audio/transcription/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/audio/transcription/#configyml","text":"# Create pipeline using lower case class name transcription : # Run pipeline with workflow workflow : transcribe : tasks : - action : transcription","title":"config.yml"},{"location":"pipeline/audio/transcription/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"transcribe\" , [ \"path to wav file\" ]))","title":"Run with Workflows"},{"location":"pipeline/audio/transcription/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"transcribe\", \"elements\":[\"path to wav file\"]}'","title":"Run with API"},{"location":"pipeline/audio/transcription/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__init__","text":"Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/audio/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__call__","text":"Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/audio/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"__call__()"},{"location":"pipeline/data/segmentation/","text":"Segmentation The Segmentation pipeline segments text into semantic units. Example The following shows a simple example using this pipeline. from txtai.pipeline import Segmentation # Create and run pipeline segment = Segmentation ( sentences = True ) segment ( \"This is a test. And another test.\" ) Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name segmentation : sentences : true # Run pipeline with workflow workflow : segment : tasks : - action : segmentation Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"segment\" , [ \"This is a test. And another test.\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"segment\", \"elements\":[\"This is a test. And another test.\"]}' Methods Python documentation for the pipeline. __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ) special Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/data/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join __call__ ( self , text ) special Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Segmentation"},{"location":"pipeline/data/segmentation/#segmentation","text":"The Segmentation pipeline segments text into semantic units.","title":"Segmentation"},{"location":"pipeline/data/segmentation/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Segmentation # Create and run pipeline segment = Segmentation ( sentences = True ) segment ( \"This is a test. And another test.\" )","title":"Example"},{"location":"pipeline/data/segmentation/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/data/segmentation/#configyml","text":"# Create pipeline using lower case class name segmentation : sentences : true # Run pipeline with workflow workflow : segment : tasks : - action : segmentation","title":"config.yml"},{"location":"pipeline/data/segmentation/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"segment\" , [ \"This is a test. And another test.\" ]))","title":"Run with Workflows"},{"location":"pipeline/data/segmentation/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"segment\", \"elements\":[\"This is a test. And another test.\"]}'","title":"Run with API"},{"location":"pipeline/data/segmentation/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__init__","text":"Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/data/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join","title":"__init__()"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__call__","text":"Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/data/tabular/","text":"Tabular The Tabular pipeline splits tabular data into rows and columns. The tabular pipeline is most useful in creating (id, text, tag) tuples to load into Embedding indexes. Example The following shows a simple example using this pipeline. from txtai.pipeline import Tabular # Create and run pipeline tabular = Tabular ( \"id\" , [ \"text\" ]) tabular ( \"path to csv file\" ) See the link below for a more detailed example. Notebook Description Transform tabular data with composable workflows Transform, index and search tabular data Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name tabular : idcolumn : id textcolumns : - text # Run pipeline with workflow workflow : tabular : tasks : - action : tabular Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"tabular\" , [ \"path to csv file\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"tabular\", \"elements\":[\"path to csv file\"]}' Methods Python documentation for the pipeline. __init__ ( self , idcolumn = None , textcolumns = None , content = False ) special Creates a new Tabular pipeline. Parameters: Name Type Description Default idcolumn column name to use for row id None textcolumns list of columns to combine as a text field None content if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. False Source code in txtai/pipeline/data/tabular.py def __init__ ( self , idcolumn = None , textcolumns = None , content = False ): \"\"\" Creates a new Tabular pipeline. Args: idcolumn: column name to use for row id textcolumns: list of columns to combine as a text field content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. \"\"\" if not PANDAS : raise ImportError ( 'Tabular pipeline is not available - install \"pipeline\" extra to enable' ) self . idcolumn = idcolumn self . textcolumns = textcolumns self . content = content __call__ ( self , data ) special Splits data into rows and columns. Parameters: Name Type Description Default data input data required Returns: Type Description list of (id, text, tag) Source code in txtai/pipeline/data/tabular.py def __call__ ( self , data ): \"\"\" Splits data into rows and columns. Args: data: input data Returns: list of (id, text, tag) \"\"\" items = [ data ] if not isinstance ( data , list ) else data # Combine all rows into single return element results = [] dicts = [] for item in items : # File path if isinstance ( item , str ): _ , extension = os . path . splitext ( item ) extension = extension . replace ( \".\" , \"\" ) . lower () if extension == \"csv\" : df = pd . read_csv ( item ) results . append ( self . process ( df )) # Dict if isinstance ( item , dict ): dicts . append ( item ) # List of dicts elif isinstance ( item , list ): df = pd . DataFrame ( item ) results . append ( self . process ( df )) if dicts : df = pd . DataFrame ( dicts ) results . extend ( self . process ( df )) return results [ 0 ] if not isinstance ( data , list ) else results","title":"Tabular"},{"location":"pipeline/data/tabular/#tabular","text":"The Tabular pipeline splits tabular data into rows and columns. The tabular pipeline is most useful in creating (id, text, tag) tuples to load into Embedding indexes.","title":"Tabular"},{"location":"pipeline/data/tabular/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Tabular # Create and run pipeline tabular = Tabular ( \"id\" , [ \"text\" ]) tabular ( \"path to csv file\" ) See the link below for a more detailed example. Notebook Description Transform tabular data with composable workflows Transform, index and search tabular data","title":"Example"},{"location":"pipeline/data/tabular/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/data/tabular/#configyml","text":"# Create pipeline using lower case class name tabular : idcolumn : id textcolumns : - text # Run pipeline with workflow workflow : tabular : tasks : - action : tabular","title":"config.yml"},{"location":"pipeline/data/tabular/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"tabular\" , [ \"path to csv file\" ]))","title":"Run with Workflows"},{"location":"pipeline/data/tabular/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"tabular\", \"elements\":[\"path to csv file\"]}'","title":"Run with API"},{"location":"pipeline/data/tabular/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__init__","text":"Creates a new Tabular pipeline. Parameters: Name Type Description Default idcolumn column name to use for row id None textcolumns list of columns to combine as a text field None content if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. False Source code in txtai/pipeline/data/tabular.py def __init__ ( self , idcolumn = None , textcolumns = None , content = False ): \"\"\" Creates a new Tabular pipeline. Args: idcolumn: column name to use for row id textcolumns: list of columns to combine as a text field content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. \"\"\" if not PANDAS : raise ImportError ( 'Tabular pipeline is not available - install \"pipeline\" extra to enable' ) self . idcolumn = idcolumn self . textcolumns = textcolumns self . content = content","title":"__init__()"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__call__","text":"Splits data into rows and columns. Parameters: Name Type Description Default data input data required Returns: Type Description list of (id, text, tag) Source code in txtai/pipeline/data/tabular.py def __call__ ( self , data ): \"\"\" Splits data into rows and columns. Args: data: input data Returns: list of (id, text, tag) \"\"\" items = [ data ] if not isinstance ( data , list ) else data # Combine all rows into single return element results = [] dicts = [] for item in items : # File path if isinstance ( item , str ): _ , extension = os . path . splitext ( item ) extension = extension . replace ( \".\" , \"\" ) . lower () if extension == \"csv\" : df = pd . read_csv ( item ) results . append ( self . process ( df )) # Dict if isinstance ( item , dict ): dicts . append ( item ) # List of dicts elif isinstance ( item , list ): df = pd . DataFrame ( item ) results . append ( self . process ( df )) if dicts : df = pd . DataFrame ( dicts ) results . extend ( self . process ( df )) return results [ 0 ] if not isinstance ( data , list ) else results","title":"__call__()"},{"location":"pipeline/data/textractor/","text":"Textractor The Textractor pipeline extracts and splits text from documents. This pipeline uses either an Apache Tika backend (if Java is available) or BeautifulSoup4 . Example The following shows a simple example using this pipeline. from txtai.pipeline import Textractor # Create and run pipeline textract = Textractor () textract ( \"https://github.com/neuml/txtai\" ) See the link below for a more detailed example. Notebook Description Extract text from documents Extract text from PDF, Office, HTML and more Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name textractor : # Run pipeline with workflow workflow : textract : tasks : - action : textractor Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"textract\" , [ \"https://github.com/neuml/txtai\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"textract\", \"elements\":[\"https://github.com/neuml/txtai\"]}' Methods Python documentation for the pipeline. __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ) special Source code in txtai/pipeline/data/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join ) # Determine if Tika (default if Java is available) or Beautiful Soup should be used # Beautiful Soup only supports HTML, Tika supports a wide variety of file formats, including HTML. self . tika = self . checkjava () if tika else False __call__ ( self , text ) special Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Textractor"},{"location":"pipeline/data/textractor/#textractor","text":"The Textractor pipeline extracts and splits text from documents. This pipeline uses either an Apache Tika backend (if Java is available) or BeautifulSoup4 .","title":"Textractor"},{"location":"pipeline/data/textractor/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Textractor # Create and run pipeline textract = Textractor () textract ( \"https://github.com/neuml/txtai\" ) See the link below for a more detailed example. Notebook Description Extract text from documents Extract text from PDF, Office, HTML and more","title":"Example"},{"location":"pipeline/data/textractor/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/data/textractor/#configyml","text":"# Create pipeline using lower case class name textractor : # Run pipeline with workflow workflow : textract : tasks : - action : textractor","title":"config.yml"},{"location":"pipeline/data/textractor/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"textract\" , [ \"https://github.com/neuml/txtai\" ]))","title":"Run with Workflows"},{"location":"pipeline/data/textractor/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"textract\", \"elements\":[\"https://github.com/neuml/txtai\"]}'","title":"Run with API"},{"location":"pipeline/data/textractor/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.textractor.Textractor.__init__","text":"Source code in txtai/pipeline/data/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join ) # Determine if Tika (default if Java is available) or Beautiful Soup should be used # Beautiful Soup only supports HTML, Tika supports a wide variety of file formats, including HTML. self . tika = self . checkjava () if tika else False","title":"__init__()"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.segmentation.Textractor.__call__","text":"Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/image/caption/","text":"Caption The caption pipeline reads a list of images and returns a list of captions for those images. Example The following shows a simple example using this pipeline. from txtai.pipeline import Caption # Create and run pipeline caption = Caption () caption ( \"path to image file\" ) See the link below for a more detailed example. Notebook Description Generate image captions and detect objects Captions and object detection for images Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name caption : # Run pipeline with workflow workflow : caption : tasks : - action : caption Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"caption\" , [ \"path to image file\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"caption\", \"elements\":[\"path to image file\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = 'ydshieh/vit-gpt2-coco-en' , quantize = False , gpu = True , batch = 64 ) special Constructs a new caption pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'ydshieh/vit-gpt2-coco-en' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/image/caption.py def __init__ ( self , path = \"ydshieh/vit-gpt2-coco-en\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new caption pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" if not PIL : raise ImportError ( 'Captions pipeline is not available - install \"pipeline\" extra to enable' ) # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # load model and processor self . model = VisionEncoderDecoderModel . from_pretrained ( self . path ) self . tokenizer = AutoTokenizer . from_pretrained ( self . path ) self . extractor = ViTFeatureExtractor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , images ) special Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Parameters: Name Type Description Default images image|list required Returns: Type Description list of captions Source code in txtai/pipeline/image/caption.py def __call__ ( self , images ): \"\"\" Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Args: images: image|list Returns: list of captions \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Feature extraction pixels = self . extractor ( images = values , return_tensors = \"pt\" ) . pixel_values pixels = pixels . to ( self . device ) # Run model with torch . no_grad (): outputs = self . model . generate ( pixels , max_length = 16 , num_beams = 4 , return_dict_in_generate = True ) . sequences # Tokenize outputs into text results captions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) captions = [ caption . strip () for caption in captions ] # Return single element if single element passed in return captions [ 0 ] if not isinstance ( images , list ) else captions","title":"Caption"},{"location":"pipeline/image/caption/#caption","text":"The caption pipeline reads a list of images and returns a list of captions for those images.","title":"Caption"},{"location":"pipeline/image/caption/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Caption # Create and run pipeline caption = Caption () caption ( \"path to image file\" ) See the link below for a more detailed example. Notebook Description Generate image captions and detect objects Captions and object detection for images","title":"Example"},{"location":"pipeline/image/caption/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/image/caption/#configyml","text":"# Create pipeline using lower case class name caption : # Run pipeline with workflow workflow : caption : tasks : - action : caption","title":"config.yml"},{"location":"pipeline/image/caption/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"caption\" , [ \"path to image file\" ]))","title":"Run with Workflows"},{"location":"pipeline/image/caption/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"caption\", \"elements\":[\"path to image file\"]}'","title":"Run with API"},{"location":"pipeline/image/caption/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__init__","text":"Constructs a new caption pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'ydshieh/vit-gpt2-coco-en' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/image/caption.py def __init__ ( self , path = \"ydshieh/vit-gpt2-coco-en\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new caption pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" if not PIL : raise ImportError ( 'Captions pipeline is not available - install \"pipeline\" extra to enable' ) # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # load model and processor self . model = VisionEncoderDecoderModel . from_pretrained ( self . path ) self . tokenizer = AutoTokenizer . from_pretrained ( self . path ) self . extractor = ViTFeatureExtractor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__call__","text":"Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Parameters: Name Type Description Default images image|list required Returns: Type Description list of captions Source code in txtai/pipeline/image/caption.py def __call__ ( self , images ): \"\"\" Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Args: images: image|list Returns: list of captions \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Feature extraction pixels = self . extractor ( images = values , return_tensors = \"pt\" ) . pixel_values pixels = pixels . to ( self . device ) # Run model with torch . no_grad (): outputs = self . model . generate ( pixels , max_length = 16 , num_beams = 4 , return_dict_in_generate = True ) . sequences # Tokenize outputs into text results captions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) captions = [ caption . strip () for caption in captions ] # Return single element if single element passed in return captions [ 0 ] if not isinstance ( images , list ) else captions","title":"__call__()"},{"location":"pipeline/image/objects/","text":"Objects The Objects pipeline reads a list of images and returns a list of detected objects. Example The following shows a simple example using this pipeline. from txtai.pipeline import Objects # Create and run pipeline objects = Objects () objects ( \"path to image file\" ) See the link below for a more detailed example. Notebook Description Generate image captions and detect objects Captions and object detection for images Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name objects : # Run pipeline with workflow workflow : objects : tasks : - action : objects Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"objects\" , [ \"path to image file\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"objects\", \"elements\":[\"path to image file\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ) special Source code in txtai/pipeline/image/objects.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ): if not PIL : raise ImportError ( 'Objects pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( \"image-classification\" if classification else \"object-detection\" , path , quantize , gpu , model ) self . classification = classification self . threshold = threshold __call__ ( self , images , flatten = False , workers = 0 ) special Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Parameters: Name Type Description Default images image|list required flatten flatten output to a list of objects False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (label, score) Source code in txtai/pipeline/image/objects.py def __call__ ( self , images , flatten = False , workers = 0 ): \"\"\" Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Args: images: image|list flatten: flatten output to a list of objects workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (label, score) \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Run pipeline results = ( self . pipeline ( values , num_workers = workers ) if self . classification else self . pipeline ( values , threshold = self . threshold , num_workers = workers ) ) # Build list of (id, score) outputs = [] for result in results : # Convert to (label, score) tuples result = [( x [ \"label\" ], x [ \"score\" ]) for x in result if x [ \"score\" ] > self . threshold ] # Sort by score descending result = sorted ( result , key = lambda x : x [ 1 ], reverse = True ) # Deduplicate labels unique = set () elements = [] for label , score in result : if label not in unique : elements . append ( label if flatten else ( label , score )) unique . add ( label ) outputs . append ( elements ) # Return single element if single element passed in return outputs [ 0 ] if not isinstance ( images , list ) else outputs","title":"Objects"},{"location":"pipeline/image/objects/#objects","text":"The Objects pipeline reads a list of images and returns a list of detected objects.","title":"Objects"},{"location":"pipeline/image/objects/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Objects # Create and run pipeline objects = Objects () objects ( \"path to image file\" ) See the link below for a more detailed example. Notebook Description Generate image captions and detect objects Captions and object detection for images","title":"Example"},{"location":"pipeline/image/objects/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/image/objects/#configyml","text":"# Create pipeline using lower case class name objects : # Run pipeline with workflow workflow : objects : tasks : - action : objects","title":"config.yml"},{"location":"pipeline/image/objects/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"objects\" , [ \"path to image file\" ]))","title":"Run with Workflows"},{"location":"pipeline/image/objects/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"objects\", \"elements\":[\"path to image file\"]}'","title":"Run with API"},{"location":"pipeline/image/objects/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__init__","text":"Source code in txtai/pipeline/image/objects.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ): if not PIL : raise ImportError ( 'Objects pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( \"image-classification\" if classification else \"object-detection\" , path , quantize , gpu , model ) self . classification = classification self . threshold = threshold","title":"__init__()"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__call__","text":"Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Parameters: Name Type Description Default images image|list required flatten flatten output to a list of objects False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (label, score) Source code in txtai/pipeline/image/objects.py def __call__ ( self , images , flatten = False , workers = 0 ): \"\"\" Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Args: images: image|list flatten: flatten output to a list of objects workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (label, score) \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Run pipeline results = ( self . pipeline ( values , num_workers = workers ) if self . classification else self . pipeline ( values , threshold = self . threshold , num_workers = workers ) ) # Build list of (id, score) outputs = [] for result in results : # Convert to (label, score) tuples result = [( x [ \"label\" ], x [ \"score\" ]) for x in result if x [ \"score\" ] > self . threshold ] # Sort by score descending result = sorted ( result , key = lambda x : x [ 1 ], reverse = True ) # Deduplicate labels unique = set () elements = [] for label , score in result : if label not in unique : elements . append ( label if flatten else ( label , score )) unique . add ( label ) outputs . append ( elements ) # Return single element if single element passed in return outputs [ 0 ] if not isinstance ( images , list ) else outputs","title":"__call__()"},{"location":"pipeline/text/entity/","text":"Entity The entity extraction pipeline applies a token classifier to text and extracts entity/label combinations. Example The following shows a simple example using this pipeline. from txtai.pipeline import Entity # Create and run pipeline entity = Entity () entity ( \"Canada's last fully intact ice shelf has suddenly collapsed, \" \\ \"forming a Manhattan-sized iceberg\" ) See the link below for a more detailed example. Notebook Description Entity extraction workflows Identify entity/label combinations Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name entity : # Run pipeline with workflow workflow : entity : tasks : - action : entity Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"entity\" , [ \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"entity\", \"elements\": [\"Canadas last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/text/entity.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"token-classification\" , path , quantize , gpu , model ) __call__ ( self , text , labels = None , aggregate = 'simple' , flatten = None , join = False , workers = 0 ) special Applies a token classifier to text and extracts entity/label combinations. Parameters: Name Type Description Default text text|list required labels list of entity type labels to accept, defaults to None which accepts all None aggregate method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\" 'simple' flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None join joins flattened output into a string if True, ignored if flatten not set False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (entity, entity type, score) or list of entities depending on flatten parameter Source code in txtai/pipeline/text/entity.py def __call__ ( self , text , labels = None , aggregate = \"simple\" , flatten = None , join = False , workers = 0 ): \"\"\" Applies a token classifier to text and extracts entity/label combinations. Args: text: text|list labels: list of entity type labels to accept, defaults to None which accepts all aggregate: method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\" flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. join: joins flattened output into a string if True, ignored if flatten not set workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (entity, entity type, score) or list of entities depending on flatten parameter \"\"\" # Run token classification pipeline results = self . pipeline ( text , aggregation_strategy = aggregate , num_workers = workers ) # Convert results to a list if necessary if isinstance ( text , str ): results = [ results ] # Score threshold when flatten is set threshold = 0.0 if isinstance ( flatten , bool ) else flatten # Extract entities if flatten set, otherwise extract (entity, entity type, score) tuples outputs = [] for result in results : if flatten : output = [ r [ \"word\" ] for r in result if self . accept ( r [ \"entity_group\" ], labels ) and r [ \"score\" ] >= threshold ] outputs . append ( \" \" . join ( output ) if join else output ) else : outputs . append ([( r [ \"word\" ], r [ \"entity_group\" ], float ( r [ \"score\" ])) for r in result if self . accept ( r [ \"entity_group\" ], labels )]) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"Entity"},{"location":"pipeline/text/entity/#entity","text":"The entity extraction pipeline applies a token classifier to text and extracts entity/label combinations.","title":"Entity"},{"location":"pipeline/text/entity/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Entity # Create and run pipeline entity = Entity () entity ( \"Canada's last fully intact ice shelf has suddenly collapsed, \" \\ \"forming a Manhattan-sized iceberg\" ) See the link below for a more detailed example. Notebook Description Entity extraction workflows Identify entity/label combinations","title":"Example"},{"location":"pipeline/text/entity/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/entity/#configyml","text":"# Create pipeline using lower case class name entity : # Run pipeline with workflow workflow : entity : tasks : - action : entity","title":"config.yml"},{"location":"pipeline/text/entity/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"entity\" , [ \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\" ]))","title":"Run with Workflows"},{"location":"pipeline/text/entity/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"entity\", \"elements\": [\"Canadas last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]}'","title":"Run with API"},{"location":"pipeline/text/entity/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/entity/#txtai.pipeline.text.entity.Entity.__init__","text":"Source code in txtai/pipeline/text/entity.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"token-classification\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipeline/text/entity/#txtai.pipeline.text.entity.Entity.__call__","text":"Applies a token classifier to text and extracts entity/label combinations. Parameters: Name Type Description Default text text|list required labels list of entity type labels to accept, defaults to None which accepts all None aggregate method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\" 'simple' flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None join joins flattened output into a string if True, ignored if flatten not set False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (entity, entity type, score) or list of entities depending on flatten parameter Source code in txtai/pipeline/text/entity.py def __call__ ( self , text , labels = None , aggregate = \"simple\" , flatten = None , join = False , workers = 0 ): \"\"\" Applies a token classifier to text and extracts entity/label combinations. Args: text: text|list labels: list of entity type labels to accept, defaults to None which accepts all aggregate: method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\" flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. join: joins flattened output into a string if True, ignored if flatten not set workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (entity, entity type, score) or list of entities depending on flatten parameter \"\"\" # Run token classification pipeline results = self . pipeline ( text , aggregation_strategy = aggregate , num_workers = workers ) # Convert results to a list if necessary if isinstance ( text , str ): results = [ results ] # Score threshold when flatten is set threshold = 0.0 if isinstance ( flatten , bool ) else flatten # Extract entities if flatten set, otherwise extract (entity, entity type, score) tuples outputs = [] for result in results : if flatten : output = [ r [ \"word\" ] for r in result if self . accept ( r [ \"entity_group\" ], labels ) and r [ \"score\" ] >= threshold ] outputs . append ( \" \" . join ( output ) if join else output ) else : outputs . append ([( r [ \"word\" ], r [ \"entity_group\" ], float ( r [ \"score\" ])) for r in result if self . accept ( r [ \"entity_group\" ], labels )]) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"__call__()"},{"location":"pipeline/text/extractor/","text":"Extractor The Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Example The following shows a simple example using this pipeline. from txtai.embeddings import Embeddings from txtai.pipeline import Extractor # Embeddings model ranks candidates before passing to QA pipeline embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Create and run pipeline extractor = Extractor ( embeddings , \"distilbert-base-cased-distilled-squad\" ) extractor ([[ \"What was won\" ] * 3 + [ False ]], [ \"Maine man wins $1M from $25 lottery ticket\" ]) See the links below for more detailed examples. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name extractor : Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . extract ([{ \"name\" : \"What was won\" , \"query\" : \"What was won\" , \"question\" , \"What was won\" , \"snippet\" : False }], [ \"Maine man wins $1M from $25 lottery ticket\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/extract\" \\ -H \"Content-Type: application/json\" \\ -d '{\"queue\": [{\"name\":\"What was won\", \"query\": \"What was won\", \"question\": \"What was won\", \"snippet\": false}], \"texts\": [\"Maine man wins $1M from $25 lottery ticket\"]}' Methods Python documentation for the pipeline. __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ) special Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None minscore minimum score to include context match, defaults to None None mintokens minimum number of tokens to include context match, defaults to None None context topn context matches to include, defaults to 3 None Source code in txtai/pipeline/text/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class minscore: minimum score to include context match, defaults to None mintokens: minimum number of tokens to include context match, defaults to None context: topn context matches to include, defaults to 3 \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0 # Top N context matches to include for question-answering self . context = context if context else 3 __call__ ( self , queue , texts ) special Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/text/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: self . context ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"Extractor"},{"location":"pipeline/text/extractor/#extractor","text":"The Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model.","title":"Extractor"},{"location":"pipeline/text/extractor/#example","text":"The following shows a simple example using this pipeline. from txtai.embeddings import Embeddings from txtai.pipeline import Extractor # Embeddings model ranks candidates before passing to QA pipeline embeddings = Embeddings ({ \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Create and run pipeline extractor = Extractor ( embeddings , \"distilbert-base-cased-distilled-squad\" ) extractor ([[ \"What was won\" ] * 3 + [ False ]], [ \"Maine man wins $1M from $25 lottery ticket\" ]) See the links below for more detailed examples. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering","title":"Example"},{"location":"pipeline/text/extractor/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/extractor/#configyml","text":"# Create pipeline using lower case class name extractor :","title":"config.yml"},{"location":"pipeline/text/extractor/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . extract ([{ \"name\" : \"What was won\" , \"query\" : \"What was won\" , \"question\" , \"What was won\" , \"snippet\" : False }], [ \"Maine man wins $1M from $25 lottery ticket\" ]))","title":"Run with Workflows"},{"location":"pipeline/text/extractor/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/extract\" \\ -H \"Content-Type: application/json\" \\ -d '{\"queue\": [{\"name\":\"What was won\", \"query\": \"What was won\", \"question\": \"What was won\", \"snippet\": false}], \"texts\": [\"Maine man wins $1M from $25 lottery ticket\"]}'","title":"Run with API"},{"location":"pipeline/text/extractor/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__init__","text":"Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None minscore minimum score to include context match, defaults to None None mintokens minimum number of tokens to include context match, defaults to None None context topn context matches to include, defaults to 3 None Source code in txtai/pipeline/text/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class minscore: minimum score to include context match, defaults to None mintokens: minimum number of tokens to include context match, defaults to None context: topn context matches to include, defaults to 3 \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0 # Top N context matches to include for question-answering self . context = context if context else 3","title":"__init__()"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__call__","text":"Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/text/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: self . context ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"__call__()"},{"location":"pipeline/text/labels/","text":"Labels The Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling). Example The following shows a simple example using this pipeline. from txtai.pipeline import Labels # Create and run pipeline labels = Labels () labels ( [ \"Great news\" , \"That's rough\" ], [ \"positive\" , \"negative\" ] ) See the link below for a more detailed example. Notebook Description Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name labels : # Run pipeline with workflow workflow : labels : tasks : - action : labels args : [[ \"positive\" , \"negative\" ]] Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"labels\" , [ \"Great news\" , \"That's rough\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"labels\", \"elements\": [\"Great news\", \"Thats rough\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ) special Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None False flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (id, score) or list of labels depending on flatten parameter Source code in txtai/pipeline/text/labels.py def __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (id, score) or list of labels depending on flatten parameter \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of outputs and return outputs = self . outputs ( results , labels , flatten ) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"Labels"},{"location":"pipeline/text/labels/#labels","text":"The Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling).","title":"Labels"},{"location":"pipeline/text/labels/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Labels # Create and run pipeline labels = Labels () labels ( [ \"Great news\" , \"That's rough\" ], [ \"positive\" , \"negative\" ] ) See the link below for a more detailed example. Notebook Description Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling","title":"Example"},{"location":"pipeline/text/labels/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/labels/#configyml","text":"# Create pipeline using lower case class name labels : # Run pipeline with workflow workflow : labels : tasks : - action : labels args : [[ \"positive\" , \"negative\" ]]","title":"config.yml"},{"location":"pipeline/text/labels/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"labels\" , [ \"Great news\" , \"That's rough\" ]))","title":"Run with Workflows"},{"location":"pipeline/text/labels/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"labels\", \"elements\": [\"Great news\", \"Thats rough\"]}'","title":"Run with API"},{"location":"pipeline/text/labels/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__init__","text":"Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__call__","text":"Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None False flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (id, score) or list of labels depending on flatten parameter Source code in txtai/pipeline/text/labels.py def __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (id, score) or list of labels depending on flatten parameter \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of outputs and return outputs = self . outputs ( results , labels , flatten ) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"__call__()"},{"location":"pipeline/text/similarity/","text":"Similarity The Similarity pipeline computes similarity between queries and list of text using a text classifier. This pipeline supports both standard text classification models and zero-shot classification models. The pipeline uses the queries as labels for the input text. The results are transposed to get scores per query/label vs scores per input text. Example The following shows a simple example using this pipeline. from txtai.pipeline import Similarity # Create and run pipeline similarity = Similarity () similarity ( \"feel good story\" , [ \"Maine man wins $1M from $25 lottery ticket\" , \"Don't sacrifice slower friends in a bear attack\" ]) See the link below for a more detailed example. Notebook Description Add semantic search to Elasticsearch Add semantic search to existing search systems Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name similarity : Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) app . similarity ( \"feel good story\" , [ \"Maine man wins $1M from $25 lottery ticket\" , \"Don't sacrifice slower friends in a bear attack\" ]) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/similarity\" \\ -H \"Content-Type: application/json\" \\ -d '{\"query\": \"feel good story\", \"texts\": [\"Maine man wins $1M from $25 lottery ticket\", \"Dont sacrifice slower friends in a bear attack\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , query , texts , multilabel = True ) special Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/text/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"Similarity"},{"location":"pipeline/text/similarity/#similarity","text":"The Similarity pipeline computes similarity between queries and list of text using a text classifier. This pipeline supports both standard text classification models and zero-shot classification models. The pipeline uses the queries as labels for the input text. The results are transposed to get scores per query/label vs scores per input text.","title":"Similarity"},{"location":"pipeline/text/similarity/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Similarity # Create and run pipeline similarity = Similarity () similarity ( \"feel good story\" , [ \"Maine man wins $1M from $25 lottery ticket\" , \"Don't sacrifice slower friends in a bear attack\" ]) See the link below for a more detailed example. Notebook Description Add semantic search to Elasticsearch Add semantic search to existing search systems","title":"Example"},{"location":"pipeline/text/similarity/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/similarity/#configyml","text":"# Create pipeline using lower case class name similarity :","title":"config.yml"},{"location":"pipeline/text/similarity/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) app . similarity ( \"feel good story\" , [ \"Maine man wins $1M from $25 lottery ticket\" , \"Don't sacrifice slower friends in a bear attack\" ])","title":"Run with Workflows"},{"location":"pipeline/text/similarity/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/similarity\" \\ -H \"Content-Type: application/json\" \\ -d '{\"query\": \"feel good story\", \"texts\": [\"Maine man wins $1M from $25 lottery ticket\", \"Dont sacrifice slower friends in a bear attack\"]}'","title":"Run with API"},{"location":"pipeline/text/similarity/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.labels.Similarity.__init__","text":"Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.similarity.Similarity.__call__","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/text/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"__call__()"},{"location":"pipeline/text/summary/","text":"Summary The Summary pipeline summarizes text. This pipeline runs a text2text model that abstractively creates a summary of the input text. Example The following shows a simple example using this pipeline. from txtai.pipeline import Summary # Create and run pipeline summary = Summary () summary ( \"Enter long, detailed text to summarize here\" ) See the link below for a more detailed example. Notebook Description Building abstractive text summaries Run abstractive text summarization Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name summary : # Run pipeline with workflow workflow : summary : tasks : - action : summary Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"summary\" , [ \"Enter long, detailed text to summarize here\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"summary\", \"elements\":[\"Enter long, detailed text to summarize here\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/text/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model ) __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ) special Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description summary text Source code in txtai/pipeline/text/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of concurrent workers to use for processing data, defaults to None Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"Summary"},{"location":"pipeline/text/summary/#summary","text":"The Summary pipeline summarizes text. This pipeline runs a text2text model that abstractively creates a summary of the input text.","title":"Summary"},{"location":"pipeline/text/summary/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Summary # Create and run pipeline summary = Summary () summary ( \"Enter long, detailed text to summarize here\" ) See the link below for a more detailed example. Notebook Description Building abstractive text summaries Run abstractive text summarization","title":"Example"},{"location":"pipeline/text/summary/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/summary/#configyml","text":"# Create pipeline using lower case class name summary : # Run pipeline with workflow workflow : summary : tasks : - action : summary","title":"config.yml"},{"location":"pipeline/text/summary/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"summary\" , [ \"Enter long, detailed text to summarize here\" ]))","title":"Run with Workflows"},{"location":"pipeline/text/summary/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"summary\", \"elements\":[\"Enter long, detailed text to summarize here\"]}'","title":"Run with API"},{"location":"pipeline/text/summary/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__init__","text":"Source code in txtai/pipeline/text/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__call__","text":"Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description summary text Source code in txtai/pipeline/text/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of concurrent workers to use for processing data, defaults to None Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/text/translation/","text":"Translation The Translation pipeline translates text between languages. It supports over 100+ languages. Automatic source language detection is built-in. This pipeline detects the language of each input text row, loads a model for the source-target combination and translates text to the target language. Example The following shows a simple example using this pipeline. from txtai.pipeline import Translation # Create and run pipeline translate = Translation () translate ( \"This is a test translation into Spanish\" , \"es\" ) See the link below for a more detailed example. Notebook Description Translate text between languages Streamline machine translation and language detection Configuration-driven example Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API . config.yml # Create pipeline using lower case class name translation : # Run pipeline with workflow workflow : translate : tasks : - action : translation args : [ \"es\" ] Run with Workflows from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"translate\" , [ \"This is a test translation into Spanish\" ])) Run with API CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"translate\", \"elements\":[\"This is a test translation into Spanish\"]}' Methods Python documentation for the pipeline. __init__ ( self , path = 'facebook/m2m100_418M' , quantize = False , gpu = True , batch = 64 , langdetect = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' ) special Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/text/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available () __call__ ( self , texts , target = 'en' , source = None ) special Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/text/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"Translation"},{"location":"pipeline/text/translation/#translation","text":"The Translation pipeline translates text between languages. It supports over 100+ languages. Automatic source language detection is built-in. This pipeline detects the language of each input text row, loads a model for the source-target combination and translates text to the target language.","title":"Translation"},{"location":"pipeline/text/translation/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import Translation # Create and run pipeline translate = Translation () translate ( \"This is a test translation into Spanish\" , \"es\" ) See the link below for a more detailed example. Notebook Description Translate text between languages Streamline machine translation and language detection","title":"Example"},{"location":"pipeline/text/translation/#configuration-driven-example","text":"Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API .","title":"Configuration-driven example"},{"location":"pipeline/text/translation/#configyml","text":"# Create pipeline using lower case class name translation : # Run pipeline with workflow workflow : translate : tasks : - action : translation args : [ \"es\" ]","title":"config.yml"},{"location":"pipeline/text/translation/#run-with-workflows","text":"from txtai.api import API # Create and run pipeline with workflow app = API ( \"config.yml\" ) list ( app . workflow ( \"translate\" , [ \"This is a test translation into Spanish\" ]))","title":"Run with Workflows"},{"location":"pipeline/text/translation/#run-with-api","text":"CONFIG = config.yml uvicorn \"txtai.api:app\" & curl \\ -X POST \"http://localhost:8000/workflow\" \\ -H \"Content-Type: application/json\" \\ -d '{\"name\":\"translate\", \"elements\":[\"This is a test translation into Spanish\"]}'","title":"Run with API"},{"location":"pipeline/text/translation/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__init__","text":"Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/text/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available ()","title":"__init__()"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__call__","text":"Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/text/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"__call__()"},{"location":"pipeline/train/hfonnx/","text":"HFOnnx Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation). Example The following shows a simple example using this pipeline. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" ) See the link below for a more detailed example. Notebook Description Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Methods Python documentation for the pipeline. __call__ ( self , path , task = 'default' , output = None , quantize = False , opset = 12 ) special Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"HF ONNX"},{"location":"pipeline/train/hfonnx/#hfonnx","text":"Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation).","title":"HFOnnx"},{"location":"pipeline/train/hfonnx/#example","text":"The following shows a simple example using this pipeline. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" ) See the link below for a more detailed example. Notebook Description Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust","title":"Example"},{"location":"pipeline/train/hfonnx/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/train/hfonnx/#txtai.pipeline.train.hfonnx.HFOnnx.__call__","text":"Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"__call__()"},{"location":"pipeline/train/mlonnx/","text":"MLOnnx Exports a traditional machine learning model (i.e. scikit-learn) to ONNX. Example See the link below for a detailed example. Notebook Description Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more Methods Python documentation for the pipeline. __call__ ( self , model , task = 'default' , output = None , opset = 12 ) special Exports a machine learning model to ONNX using ONNXMLTools. Parameters: Name Type Description Default model model to export required task optional model task or category 'default' output optional output model path, defaults to return byte array if None None opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/mlonnx.py def __call__ ( self , model , task = \"default\" , output = None , opset = 12 ): \"\"\" Exports a machine learning model to ONNX using ONNXMLTools. Args: model: model to export task: optional model task or category output: optional output model path, defaults to return byte array if None opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" # Convert scikit-learn model to ONNX model = convert_sklearn ( model , task , initial_types = [( \"input_ids\" , StringTensorType ([ None , None ]))], target_opset = opset ) # Prune model graph down to only output probabilities model = select_model_inputs_outputs ( model , outputs = \"probabilities\" ) # pylint: disable=E1101 # Rename output to logits for consistency with other models model . graph . output [ 0 ] . name = \"logits\" model . graph . node [ 0 ] . output [ 0 ] = \"logits\" # Save model to specified output path or return bytes model = save_onnx_model ( model , output ) return output if output else model","title":"ML ONNX"},{"location":"pipeline/train/mlonnx/#mlonnx","text":"Exports a traditional machine learning model (i.e. scikit-learn) to ONNX.","title":"MLOnnx"},{"location":"pipeline/train/mlonnx/#example","text":"See the link below for a detailed example. Notebook Description Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more","title":"Example"},{"location":"pipeline/train/mlonnx/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/train/mlonnx/#txtai.pipeline.train.mlonnx.MLOnnx.__call__","text":"Exports a machine learning model to ONNX using ONNXMLTools. Parameters: Name Type Description Default model model to export required task optional model task or category 'default' output optional output model path, defaults to return byte array if None None opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/mlonnx.py def __call__ ( self , model , task = \"default\" , output = None , opset = 12 ): \"\"\" Exports a machine learning model to ONNX using ONNXMLTools. Args: model: model to export task: optional model task or category output: optional output model path, defaults to return byte array if None opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" # Convert scikit-learn model to ONNX model = convert_sklearn ( model , task , initial_types = [( \"input_ids\" , StringTensorType ([ None , None ]))], target_opset = opset ) # Prune model graph down to only output probabilities model = select_model_inputs_outputs ( model , outputs = \"probabilities\" ) # pylint: disable=E1101 # Rename output to logits for consistency with other models model . graph . output [ 0 ] . name = \"logits\" model . graph . node [ 0 ] . output [ 0 ] = \"logits\" # Save model to specified output path or return bytes model = save_onnx_model ( model , output ) return output if output else model","title":"__call__()"},{"location":"pipeline/train/trainer/","text":"HFTrainer Trains a new Hugging Face Transformer model using the Trainer framework. Example The following shows a simple example using this pipeline. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call. See the links below for more detailed examples. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Methods Python documentation for the pipeline. __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = 'text-classification' , ** args ) special Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/train/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"Trainer"},{"location":"pipeline/train/trainer/#hftrainer","text":"Trains a new Hugging Face Transformer model using the Trainer framework.","title":"HFTrainer"},{"location":"pipeline/train/trainer/#example","text":"The following shows a simple example using this pipeline. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call. See the links below for more detailed examples. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models","title":"Example"},{"location":"pipeline/train/trainer/#methods","text":"Python documentation for the pipeline.","title":"Methods"},{"location":"pipeline/train/trainer/#txtai.pipeline.train.hftrainer.HFTrainer.__call__","text":"Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/train/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"__call__()"},{"location":"workflow/","text":"Workflow Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming and work on data in batches, allowing large volumes of data to be processed efficiently. Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Transformers models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. An example of the most basic workflow: workflow = Workflow ([ Task ( lambda x : [ y * 2 for y in x ])]) list ( workflow ([ 1 , 2 , 3 ])) This example simply multiplies each input value and returns a outputs via a generator. Workflows are run with Python or configuration. Examples of both methods are shown below. Example A full-featured example is shown below in Python. This workflow transcribes a set of audio files, translates the text into French and indexes the data. from txtai.embeddings import Embeddings from txtai.pipeline import Transcription , Translation from txtai.workflow import FileTask , Task , Workflow # Embeddings instance embeddings = Embeddings ({ \"path\" : \"sentence-transformers/paraphrase-MiniLM-L3-v2\" , \"content\" : True }) # Transcription instance transcribe = Transcription () # Translation instance translate = Translation () tasks = [ FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )) ] # List of files to process data = [ \"US_tops_5_million.wav\" , \"Canadas_last_fully.wav\" , \"Beijing_mobilises.wav\" , \"The_National_Park.wav\" , \"Maine_man_wins_1_mil.wav\" , \"Make_huge_profits.wav\" ] # Workflow that translate text to French workflow = Workflow ( tasks ) # Index data embeddings . index (( uid , text , None ) for uid , text in enumerate ( workflow ( data ))) # Search embeddings . search ( \"wildlife\" , 1 ) Configuration-driven example Workflows can be defined using Python as shown above but they can also run with YAML configuration. writable : true embeddings : path : sentence-transformers/paraphrase-MiniLM-L3-v2 content : true # Transcribe audio to text transcription : # Translate text between languages translation : workflow : index : tasks : - action : transcription select : \"\\\\.wav$\" task : file - action : translation args : [ \"fr\" ] - action : index # Create and run the workflow from txtai.api import API # Create and run the workflow app = API ( \"workflow.yml\" ) list ( app . workflow ( \"index\" , [ \"US_tops_5_million.wav\" , \"Canadas_last_fully.wav\" , \"Beijing_mobilises.wav\" , \"The_National_Park.wav\" , \"Maine_man_wins_1_mil.wav\" , \"Make_huge_profits.wav\" ])) # Search app . search ( \"wildlife\" ) The code above executes a workflow defined in the file workflow.yml . The API is used to run the workflow locally, there is minimal overhead running workflows in this manner. It's a matter of preference. See the following links for more information. Workflow Demo Workflow YAML Examples Workflow YAML Guide Methods Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements. __init__ ( self , tasks , batch = 100 , workers = None , name = None ) special Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 workers number of concurrent workers None name workflow name None Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 , workers = None , name = None ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 workers: number of concurrent workers name: workflow name \"\"\" self . tasks = tasks self . batch = batch self . workers = workers self . name = name # Set default number of executor workers to max number of actions in a task self . workers = max ( len ( task . action ) for task in self . tasks ) if not self . workers else self . workers __call__ ( self , elements ) special Executes a workflow for input elements. Parameters: Name Type Description Default elements iterable data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: iterable data elements Returns: transformed data elements \"\"\" # Create execute instance for this run with Execute ( self . workers ) as executor : # Run task initializers self . initialize () # Process elements in batches for batch in self . chunk ( elements ): yield from self . process ( batch , executor ) # Run task finalizers self . finalize () schedule ( self , cron , elements , iterations = None ) Schedules a workflow using a cron expression and elements. Parameters: Name Type Description Default cron cron expression required elements iterable data elements passed to workflow each call required iterations number of times to run workflow, defaults to run indefinitely None Source code in txtai/workflow/base.py def schedule ( self , cron , elements , iterations = None ): \"\"\" Schedules a workflow using a cron expression and elements. Args: cron: cron expression elements: iterable data elements passed to workflow each call iterations: number of times to run workflow, defaults to run indefinitely \"\"\" # Check that croniter is installed if not CRONITER : raise ImportError ( 'Workflow scheduling is not available - install \"workflow\" extra to enable' ) logger . info ( \"' %s ' scheduler started with schedule %s \" , self . name , cron ) maxiterations = iterations while iterations is None or iterations > 0 : # Schedule using localtime schedule = croniter ( cron , datetime . now () . astimezone ()) . get_next ( datetime ) logger . info ( \"' %s ' next run scheduled for %s \" , self . name , schedule . isoformat ()) time . sleep ( schedule . timestamp () - time . time ()) # Run workflow # pylint: disable=W0703 try : for _ in self ( elements ): pass except Exception : logger . error ( traceback . format_exc ()) # Decrement iterations remaining, if necessary if iterations is not None : iterations -= 1 logger . info ( \"' %s ' max iterations ( %d ) reached\" , self . name , maxiterations ) More examples See this link for a full list of workflow examples.","title":"Workflow"},{"location":"workflow/#workflow","text":"Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming and work on data in batches, allowing large volumes of data to be processed efficiently. Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Transformers models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. An example of the most basic workflow: workflow = Workflow ([ Task ( lambda x : [ y * 2 for y in x ])]) list ( workflow ([ 1 , 2 , 3 ])) This example simply multiplies each input value and returns a outputs via a generator. Workflows are run with Python or configuration. Examples of both methods are shown below.","title":"Workflow"},{"location":"workflow/#example","text":"A full-featured example is shown below in Python. This workflow transcribes a set of audio files, translates the text into French and indexes the data. from txtai.embeddings import Embeddings from txtai.pipeline import Transcription , Translation from txtai.workflow import FileTask , Task , Workflow # Embeddings instance embeddings = Embeddings ({ \"path\" : \"sentence-transformers/paraphrase-MiniLM-L3-v2\" , \"content\" : True }) # Transcription instance transcribe = Transcription () # Translation instance translate = Translation () tasks = [ FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )) ] # List of files to process data = [ \"US_tops_5_million.wav\" , \"Canadas_last_fully.wav\" , \"Beijing_mobilises.wav\" , \"The_National_Park.wav\" , \"Maine_man_wins_1_mil.wav\" , \"Make_huge_profits.wav\" ] # Workflow that translate text to French workflow = Workflow ( tasks ) # Index data embeddings . index (( uid , text , None ) for uid , text in enumerate ( workflow ( data ))) # Search embeddings . search ( \"wildlife\" , 1 )","title":"Example"},{"location":"workflow/#configuration-driven-example","text":"Workflows can be defined using Python as shown above but they can also run with YAML configuration. writable : true embeddings : path : sentence-transformers/paraphrase-MiniLM-L3-v2 content : true # Transcribe audio to text transcription : # Translate text between languages translation : workflow : index : tasks : - action : transcription select : \"\\\\.wav$\" task : file - action : translation args : [ \"fr\" ] - action : index # Create and run the workflow from txtai.api import API # Create and run the workflow app = API ( \"workflow.yml\" ) list ( app . workflow ( \"index\" , [ \"US_tops_5_million.wav\" , \"Canadas_last_fully.wav\" , \"Beijing_mobilises.wav\" , \"The_National_Park.wav\" , \"Maine_man_wins_1_mil.wav\" , \"Make_huge_profits.wav\" ])) # Search app . search ( \"wildlife\" ) The code above executes a workflow defined in the file workflow.yml . The API is used to run the workflow locally, there is minimal overhead running workflows in this manner. It's a matter of preference. See the following links for more information. Workflow Demo Workflow YAML Examples Workflow YAML Guide","title":"Configuration-driven example"},{"location":"workflow/#methods","text":"Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements.","title":"Methods"},{"location":"workflow/#txtai.workflow.base.Workflow.__init__","text":"Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 workers number of concurrent workers None name workflow name None Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 , workers = None , name = None ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 workers: number of concurrent workers name: workflow name \"\"\" self . tasks = tasks self . batch = batch self . workers = workers self . name = name # Set default number of executor workers to max number of actions in a task self . workers = max ( len ( task . action ) for task in self . tasks ) if not self . workers else self . workers","title":"__init__()"},{"location":"workflow/#txtai.workflow.base.Workflow.__call__","text":"Executes a workflow for input elements. Parameters: Name Type Description Default elements iterable data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: iterable data elements Returns: transformed data elements \"\"\" # Create execute instance for this run with Execute ( self . workers ) as executor : # Run task initializers self . initialize () # Process elements in batches for batch in self . chunk ( elements ): yield from self . process ( batch , executor ) # Run task finalizers self . finalize ()","title":"__call__()"},{"location":"workflow/#txtai.workflow.base.Workflow.schedule","text":"Schedules a workflow using a cron expression and elements. Parameters: Name Type Description Default cron cron expression required elements iterable data elements passed to workflow each call required iterations number of times to run workflow, defaults to run indefinitely None Source code in txtai/workflow/base.py def schedule ( self , cron , elements , iterations = None ): \"\"\" Schedules a workflow using a cron expression and elements. Args: cron: cron expression elements: iterable data elements passed to workflow each call iterations: number of times to run workflow, defaults to run indefinitely \"\"\" # Check that croniter is installed if not CRONITER : raise ImportError ( 'Workflow scheduling is not available - install \"workflow\" extra to enable' ) logger . info ( \"' %s ' scheduler started with schedule %s \" , self . name , cron ) maxiterations = iterations while iterations is None or iterations > 0 : # Schedule using localtime schedule = croniter ( cron , datetime . now () . astimezone ()) . get_next ( datetime ) logger . info ( \"' %s ' next run scheduled for %s \" , self . name , schedule . isoformat ()) time . sleep ( schedule . timestamp () - time . time ()) # Run workflow # pylint: disable=W0703 try : for _ in self ( elements ): pass except Exception : logger . error ( traceback . format_exc ()) # Decrement iterations remaining, if necessary if iterations is not None : iterations -= 1 logger . info ( \"' %s ' max iterations ( %d ) reached\" , self . name , maxiterations )","title":"schedule()"},{"location":"workflow/#more-examples","text":"See this link for a full list of workflow examples.","title":"More examples"},{"location":"workflow/schedule/","text":"Schedule Workflows can run on a repeating basis with schedules. This is suitable in cases where a workflow is run against a dynamically expanding input, like an API service or directory of files. The schedule method takes a cron expression, list of static elements (which dynamically expand i.e. API service, directory listing) and an optional maximum number of iterations. Below are a couple example cron expressions. # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # | | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) # | | | | | \u250c\u2500\u2500\u2500\u2500\u2500 second (0 - 59) # | | | | | | * * * * * * # Run every second 0 /5 * * * * # Run every 5 minutes 0 0 1 * * # Run monthly on 1st 0 0 1 1 * # Run on Jan 1 at 12am 0 0 * * mon,wed # Run Monday and Wednesday Python Simple workflow scheduled with Python. workflow = Workflow ( tasks ) workflow . schedule ( \"0/5 * * * *\" , elements ) See the link below for a more detailed example. Notebook Description Workflow Scheduling Schedule workflows with cron expressions Configuration Simple workflow scheduled with configuration. workflow : index : schedule : cron : 0/5 * * * * elements : [ ... ] tasks : [ ... ] # Create and run the workflow from txtai.api import API # Create and run the workflow app = API ( \"workflow.yml\" ) # Wait for scheduled workflows app . wait () See the links below for more information on cron expressions. cron overview croniter - library used by txtai","title":"Schedule"},{"location":"workflow/schedule/#schedule","text":"Workflows can run on a repeating basis with schedules. This is suitable in cases where a workflow is run against a dynamically expanding input, like an API service or directory of files. The schedule method takes a cron expression, list of static elements (which dynamically expand i.e. API service, directory listing) and an optional maximum number of iterations. Below are a couple example cron expressions. # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # | | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) # | | | | | \u250c\u2500\u2500\u2500\u2500\u2500 second (0 - 59) # | | | | | | * * * * * * # Run every second 0 /5 * * * * # Run every 5 minutes 0 0 1 * * # Run monthly on 1st 0 0 1 1 * # Run on Jan 1 at 12am 0 0 * * mon,wed # Run Monday and Wednesday","title":"Schedule"},{"location":"workflow/schedule/#python","text":"Simple workflow scheduled with Python. workflow = Workflow ( tasks ) workflow . schedule ( \"0/5 * * * *\" , elements ) See the link below for a more detailed example. Notebook Description Workflow Scheduling Schedule workflows with cron expressions","title":"Python"},{"location":"workflow/schedule/#configuration","text":"Simple workflow scheduled with configuration. workflow : index : schedule : cron : 0/5 * * * * elements : [ ... ] tasks : [ ... ] # Create and run the workflow from txtai.api import API # Create and run the workflow app = API ( \"workflow.yml\" ) # Wait for scheduled workflows app . wait () See the links below for more information on cron expressions. cron overview croniter - library used by txtai","title":"Configuration"},{"location":"workflow/task/","text":"Tasks Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. While similar to pipelines, tasks encapsulate processing and don't perform signficant transformations on their own. Tasks perform logic to prepare content for the underlying action(s). A simple task is shown below. Task ( lambda x : [ y * 2 for y in x ]) The task above executes the function above for all input elements. Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element. summary = Summary () Task ( summary ) Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing. summary = Summary () task = Task ( summary ) task ([ \"Very long text here\" ]) workflow = Workflow ([ task ]) list ( workflow ([ \"Very long text here\" ])) Tasks can also be created with configuration as part of a workflow. workflow : tasks : - action : summary __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Parameters: Name Type Description Default action action(s) to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True column column index to select if element is a tuple, defaults to all None merge merge mode for joining multi-action outputs, defaults to hstack 'hstack' initialize action to execute before processing None finalize action to execute after processing None concurrency sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency None kwargs additional keyword arguments {} Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) Multi-action task concurrency The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks. In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below. multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions. multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions. More information on multiprocessing can be found in the Python documentation . Multi-action task merges Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways. hstack ( self , outputs ) Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as tuples (column-wise) Source code in txtai/workflow/task/base.py def hstack ( self , outputs ): \"\"\" Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as tuples (column-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . stack ( outputs , axis = 1 ) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . stack ( outputs , axis = 1 ) return list ( zip ( * outputs )) vstack ( self , outputs ) Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as one to many transforms (row-wise) Source code in txtai/workflow/task/base.py def vstack ( self , outputs ): \"\"\" Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as one to many transforms (row-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . concatenate ( np . stack ( outputs , axis = 1 )) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . cat ( tuple ( torch . stack ( outputs , axis = 1 ))) # Flatten into lists of outputs per input row. Wrap as one to many transformation. merge = [] for x in zip ( * outputs ): combine = [] for y in x : if isinstance ( y , list ): combine . extend ( y ) else : combine . append ( y ) merge . append ( OneToMany ( combine )) return merge concat ( self , outputs ) Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of concat outputs Source code in txtai/workflow/task/base.py def concat ( self , outputs ): \"\"\" Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Args: outputs: task outputs Returns: list of concat outputs \"\"\" return [ \". \" . join ([ str ( y ) for y in x if y ]) for x in self . hstack ( outputs )] Extract task output columns With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element. A simple example: workflow = Workflow ([ Task ( lambda x : [ y * 3 for y in x ], unpack = False , column = 0 )]) list ( workflow ([( 2 , 8 )])) For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. workflow = Workflow ([ Task ([ lambda x : [ y * 3 for y in x ], lambda x : [ y - 1 for y in x ]], unpack = False , column = { 0 : 0 , 1 : 1 })]) list ( workflow ([( 2 , 8 )])) The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!","title":"Tasks"},{"location":"workflow/task/#tasks","text":"Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. While similar to pipelines, tasks encapsulate processing and don't perform signficant transformations on their own. Tasks perform logic to prepare content for the underlying action(s). A simple task is shown below. Task ( lambda x : [ y * 2 for y in x ]) The task above executes the function above for all input elements. Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element. summary = Summary () Task ( summary ) Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing. summary = Summary () task = Task ( summary ) task ([ \"Very long text here\" ]) workflow = Workflow ([ task ]) list ( workflow ([ \"Very long text here\" ])) Tasks can also be created with configuration as part of a workflow. workflow : tasks : - action : summary","title":"Tasks"},{"location":"workflow/task/#txtai.workflow.task.base.Task.__init__","text":"Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Parameters: Name Type Description Default action action(s) to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True column column index to select if element is a tuple, defaults to all None merge merge mode for joining multi-action outputs, defaults to hstack 'hstack' initialize action to execute before processing None finalize action to execute after processing None concurrency sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency None kwargs additional keyword arguments {} Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/#multi-action-task-concurrency","text":"The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks. In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below. multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions. multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions. More information on multiprocessing can be found in the Python documentation .","title":"Multi-action task concurrency"},{"location":"workflow/task/#multi-action-task-merges","text":"Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways.","title":"Multi-action task merges"},{"location":"workflow/task/#txtai.workflow.task.base.Task.hstack","text":"Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as tuples (column-wise) Source code in txtai/workflow/task/base.py def hstack ( self , outputs ): \"\"\" Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as tuples (column-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . stack ( outputs , axis = 1 ) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . stack ( outputs , axis = 1 ) return list ( zip ( * outputs ))","title":"hstack()"},{"location":"workflow/task/#txtai.workflow.task.base.Task.vstack","text":"Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as one to many transforms (row-wise) Source code in txtai/workflow/task/base.py def vstack ( self , outputs ): \"\"\" Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as one to many transforms (row-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . concatenate ( np . stack ( outputs , axis = 1 )) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . cat ( tuple ( torch . stack ( outputs , axis = 1 ))) # Flatten into lists of outputs per input row. Wrap as one to many transformation. merge = [] for x in zip ( * outputs ): combine = [] for y in x : if isinstance ( y , list ): combine . extend ( y ) else : combine . append ( y ) merge . append ( OneToMany ( combine )) return merge","title":"vstack()"},{"location":"workflow/task/#txtai.workflow.task.base.Task.concat","text":"Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of concat outputs Source code in txtai/workflow/task/base.py def concat ( self , outputs ): \"\"\" Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Args: outputs: task outputs Returns: list of concat outputs \"\"\" return [ \". \" . join ([ str ( y ) for y in x if y ]) for x in self . hstack ( outputs )]","title":"concat()"},{"location":"workflow/task/#extract-task-output-columns","text":"With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element. A simple example: workflow = Workflow ([ Task ( lambda x : [ y * 3 for y in x ], unpack = False , column = 0 )]) list ( workflow ([( 2 , 8 )])) For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. workflow = Workflow ([ Task ([ lambda x : [ y * 3 for y in x ], lambda x : [ y - 1 for y in x ]], unpack = False , column = { 0 : 0 , 1 : 1 })]) list ( workflow ([( 2 , 8 )])) The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!","title":"Extract task output columns"},{"location":"workflow/task/console/","text":"Console Task The Console Task prints task inputs and outputs to standard output. This task is mainly used for debugging and can be added at any point in a workflow. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ ConsoleTask ()]) workflow ([ \"Input 1\" , \"Input2\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : console Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Console"},{"location":"workflow/task/console/#console-task","text":"The Console Task prints task inputs and outputs to standard output. This task is mainly used for debugging and can be added at any point in a workflow.","title":"Console Task"},{"location":"workflow/task/console/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ ConsoleTask ()]) workflow ([ \"Input 1\" , \"Input2\" ])","title":"Example"},{"location":"workflow/task/console/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : console","title":"Configuration-driven example"},{"location":"workflow/task/console/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/console/#txtai.workflow.task.base.ConsoleTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/export/","text":"Export Task The Export Task exports task outputs to CSV or Excel. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ ExportTask ()]) workflow ([ \"Input 1\" , \"Input2\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : export Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) register ( self , output = None , timestamp = None ) Add export parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default output output file path None timestamp true if output file should be timestamped None Source code in txtai/workflow/task/export.py def register ( self , output = None , timestamp = None ): \"\"\" Add export parameters to task. Checks if required dependencies are installed. Args: output: output file path timestamp: true if output file should be timestamped \"\"\" if not PANDAS : raise ImportError ( 'ExportTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 self . output = output self . timestamp = timestamp","title":"Export"},{"location":"workflow/task/export/#export-task","text":"The Export Task exports task outputs to CSV or Excel.","title":"Export Task"},{"location":"workflow/task/export/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ ExportTask ()]) workflow ([ \"Input 1\" , \"Input2\" ])","title":"Example"},{"location":"workflow/task/export/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : export","title":"Configuration-driven example"},{"location":"workflow/task/export/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/export/#txtai.workflow.task.base.ExportTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/export/#txtai.workflow.task.export.ExportTask.register","text":"Add export parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default output output file path None timestamp true if output file should be timestamped None Source code in txtai/workflow/task/export.py def register ( self , output = None , timestamp = None ): \"\"\" Add export parameters to task. Checks if required dependencies are installed. Args: output: output file path timestamp: true if output file should be timestamped \"\"\" if not PANDAS : raise ImportError ( 'ExportTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 self . output = output self . timestamp = timestamp","title":"register()"},{"location":"workflow/task/file/","text":"File Task The File Task validates a file exists. It handles both file paths and local file urls. Note that this task only works with local files. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ FileTask ()]) workflow ([ \"/path/to/file\" , \"file:///path/to/file\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : file Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"File"},{"location":"workflow/task/file/#file-task","text":"The File Task validates a file exists. It handles both file paths and local file urls. Note that this task only works with local files.","title":"File Task"},{"location":"workflow/task/file/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import FileTask , Workflow workflow = Workflow ([ FileTask ()]) workflow ([ \"/path/to/file\" , \"file:///path/to/file\" ])","title":"Example"},{"location":"workflow/task/file/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : file","title":"Configuration-driven example"},{"location":"workflow/task/file/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/file/#txtai.workflow.task.base.FileTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/image/","text":"Image Task The Image Task reads file paths, check the file is an image and opens it as an Image object. Note that this task only works with local files. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import ImageTask , Workflow workflow = Workflow ([ ImageTask ()]) workflow ([ \"image.jpg\" , \"image.gif\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : image Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Image"},{"location":"workflow/task/image/#image-task","text":"The Image Task reads file paths, check the file is an image and opens it as an Image object. Note that this task only works with local files.","title":"Image Task"},{"location":"workflow/task/image/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import ImageTask , Workflow workflow = Workflow ([ ImageTask ()]) workflow ([ \"image.jpg\" , \"image.gif\" ])","title":"Example"},{"location":"workflow/task/image/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : image","title":"Configuration-driven example"},{"location":"workflow/task/image/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/image/#txtai.workflow.task.base.ImageTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/retrieve/","text":"Retrieve Task The Retrieve Task connects to a url and downloads the content locally. This task is helpful when working with actions that require data to be available locally. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import RetrieveTask , Workflow workflow = Workflow ([ RetrieveTask ( directory = \"/tmp\" )]) workflow ([ \"https://file.to.download\" , \"/local/file/to/copy\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : retrieve directory : /tmp Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) register ( self , directory = None ) Adds retrieve parameters to task. Parameters: Name Type Description Default directory local directory used to store retrieved files None Source code in txtai/workflow/task/retrieve.py def register ( self , directory = None ): \"\"\" Adds retrieve parameters to task. Args: directory: local directory used to store retrieved files \"\"\" # pylint: disable=W0201 # Create default temporary directory if not specified if not directory : # Save tempdir to prevent content from being deleted until this task is out of scope # pylint: disable=R1732 self . tempdir = tempfile . TemporaryDirectory () directory = self . tempdir . name # Create output directory if necessary if not os . path . exists ( directory ): os . makedirs ( directory ) self . directory = directory","title":"Retrieve"},{"location":"workflow/task/retrieve/#retrieve-task","text":"The Retrieve Task connects to a url and downloads the content locally. This task is helpful when working with actions that require data to be available locally.","title":"Retrieve Task"},{"location":"workflow/task/retrieve/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import RetrieveTask , Workflow workflow = Workflow ([ RetrieveTask ( directory = \"/tmp\" )]) workflow ([ \"https://file.to.download\" , \"/local/file/to/copy\" ])","title":"Example"},{"location":"workflow/task/retrieve/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : retrieve directory : /tmp","title":"Configuration-driven example"},{"location":"workflow/task/retrieve/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/retrieve/#txtai.workflow.task.base.RetrieveTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/retrieve/#txtai.workflow.task.retrieve.RetrieveTask.register","text":"Adds retrieve parameters to task. Parameters: Name Type Description Default directory local directory used to store retrieved files None Source code in txtai/workflow/task/retrieve.py def register ( self , directory = None ): \"\"\" Adds retrieve parameters to task. Args: directory: local directory used to store retrieved files \"\"\" # pylint: disable=W0201 # Create default temporary directory if not specified if not directory : # Save tempdir to prevent content from being deleted until this task is out of scope # pylint: disable=R1732 self . tempdir = tempfile . TemporaryDirectory () directory = self . tempdir . name # Create output directory if necessary if not os . path . exists ( directory ): os . makedirs ( directory ) self . directory = directory","title":"register()"},{"location":"workflow/task/service/","text":"Service Task The Service Task extracts content from a http service. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import ServiceTask , Workflow workflow = Workflow ([ ServiceTask ( url = \"https://service.url/action)]) workflow ([ \"parameter\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : service url : https://service.url/action Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) register ( self , url = None , method = None , params = None , batch = True , extract = None ) Adds service parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default url url to connect to None method http method, GET or POST None params default query parameters None batch if True, all elements are passed in a single batch request, otherwise a service call is executed per element True extract list of sections to extract from response None Source code in txtai/workflow/task/service.py def register ( self , url = None , method = None , params = None , batch = True , extract = None ): \"\"\" Adds service parameters to task. Checks if required dependencies are installed. Args: url: url to connect to method: http method, GET or POST params: default query parameters batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element extract: list of sections to extract from response \"\"\" if not XML_TO_DICT : raise ImportError ( 'ServiceTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 # Save URL, method and parameter defaults self . url = url self . method = method self . params = params # If True, all elements are passed in a single batch request, otherwise a service call is executed per element self . batch = batch # Save sections to extract. Supports both a single string and a hierarchical list of sections. self . extract = extract if self . extract : self . extract = [ self . extract ] if isinstance ( self . extract , str ) else self . extract","title":"Service"},{"location":"workflow/task/service/#service-task","text":"The Service Task extracts content from a http service.","title":"Service Task"},{"location":"workflow/task/service/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import ServiceTask , Workflow workflow = Workflow ([ ServiceTask ( url = \"https://service.url/action)]) workflow ([ \"parameter\" ])","title":"Example"},{"location":"workflow/task/service/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : service url : https://service.url/action","title":"Configuration-driven example"},{"location":"workflow/task/service/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/service/#txtai.workflow.task.base.ServiceTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/service/#txtai.workflow.task.service.ServiceTask.register","text":"Adds service parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default url url to connect to None method http method, GET or POST None params default query parameters None batch if True, all elements are passed in a single batch request, otherwise a service call is executed per element True extract list of sections to extract from response None Source code in txtai/workflow/task/service.py def register ( self , url = None , method = None , params = None , batch = True , extract = None ): \"\"\" Adds service parameters to task. Checks if required dependencies are installed. Args: url: url to connect to method: http method, GET or POST params: default query parameters batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element extract: list of sections to extract from response \"\"\" if not XML_TO_DICT : raise ImportError ( 'ServiceTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 # Save URL, method and parameter defaults self . url = url self . method = method self . params = params # If True, all elements are passed in a single batch request, otherwise a service call is executed per element self . batch = batch # Save sections to extract. Supports both a single string and a hierarchical list of sections. self . extract = extract if self . extract : self . extract = [ self . extract ] if isinstance ( self . extract , str ) else self . extract","title":"register()"},{"location":"workflow/task/storage/","text":"Storage Task The Storage Task expands a local directory or cloud storage bucket into a list of URLs to process. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import StorageTask , Workflow workflow = Workflow ([ StorageTask ()]) workflow ([ \"s3://path/to/bucket\" , \"local://local/directory\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : storage Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Storage"},{"location":"workflow/task/storage/#storage-task","text":"The Storage Task expands a local directory or cloud storage bucket into a list of URLs to process.","title":"Storage Task"},{"location":"workflow/task/storage/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import StorageTask , Workflow workflow = Workflow ([ StorageTask ()]) workflow ([ \"s3://path/to/bucket\" , \"local://local/directory\" ])","title":"Example"},{"location":"workflow/task/storage/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : storage","title":"Configuration-driven example"},{"location":"workflow/task/storage/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/storage/#txtai.workflow.task.base.StorageTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/url/","text":"Url Task The Url Task validates that inputs start with a url prefix. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import UrlTask , Workflow workflow = Workflow ([ UrlTask ()]) workflow ([ \"https://file.to.download\" , \"file:////local/file/to/copy\" ]) Configuration-driven example This task can also be created with workflow configuration. workflow : tasks : - task : url Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Url"},{"location":"workflow/task/url/#url-task","text":"The Url Task validates that inputs start with a url prefix.","title":"Url Task"},{"location":"workflow/task/url/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import UrlTask , Workflow workflow = Workflow ([ UrlTask ()]) workflow ([ \"https://file.to.download\" , \"file:////local/file/to/copy\" ])","title":"Example"},{"location":"workflow/task/url/#configuration-driven-example","text":"This task can also be created with workflow configuration. workflow : tasks : - task : url","title":"Configuration-driven example"},{"location":"workflow/task/url/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/url/#txtai.workflow.task.base.UrlTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/workflow/","text":"Workflow Task The Workflow Task runs a workflow. Allows creating workflows of workflows. Example The following shows a simple example using this task as part of a workflow. from txtai.workflow import WorkflowTask , Workflow workflow = Workflow ([ WorkflowTask ( otherworkflow )]) workflow ([ \"input data\" ]) Methods Python documentation for the task. __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Workflow"},{"location":"workflow/task/workflow/#workflow-task","text":"The Workflow Task runs a workflow. Allows creating workflows of workflows.","title":"Workflow Task"},{"location":"workflow/task/workflow/#example","text":"The following shows a simple example using this task as part of a workflow. from txtai.workflow import WorkflowTask , Workflow workflow = Workflow ([ WorkflowTask ( otherworkflow )]) workflow ([ \"input data\" ])","title":"Example"},{"location":"workflow/task/workflow/#methods","text":"Python documentation for the task.","title":"Methods"},{"location":"workflow/task/workflow/#txtai.workflow.task.base.WorkflowTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"}]}